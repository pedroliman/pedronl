---
title: Factory Physics' Flow Benchmarking in R (with Metamodeling)
author: Pedro N. de Lima
date: '2019-04-20'
slug: factory-physics-flow-benchmarking-in-r
categories:
  - R Blogs
  - R
tags:
  - R
  - Simulation
  - Arena
  - Arena2r
header:
  caption: ''
  image: '/post/2019-data/many-objective-inventory-tradeoffs.gif'
bibliography: [references/references-metamodeling.bib]
link-citations: true
---

Teaching MBA / MBE classes to seasoned managers is not easy, specially to young lecturers like me. At the same time, it's necessary to bring new thoughts to the table, and to provide a fundamental basis for experienced people with diverse backgrounds. One of the strategies I found to overcome these obstacles this week was to use a new analysis framework (in my case, Factory Physics concepts) and use that to challenge their views about existing frames they already master (in my case, Lean Manufacturing and Theory of Constraints). Also, using some advanced analytics and lively visuals (like the many-objective parallel plot below) helped to get the course going.

![](/post/2019-data/many-objective-inventory-tradeoffs.gif) 

Turns out, this really worked well! This post shares some of the [R](https://www.r-project.org/) code I developed while putting together course materials and is an illustration of how to use [simulation metamodeling](/post/des-metamodeling-splines-r-arena/) and Arena to Run Factory Physics' Flow Benchmarking.

## Flow Benchmarking

[Flow Benchmarking](https://factoryphysics.com/flow-benchmarking) is an absolute benchmarking technique useful to determine how close a production flow is to its best possible performance. The technique has been introduced in the award-winning *Factory Physics* (FP) Book [@hopp2008factory], and is a key component of the science-based manufacturing management approach described in [@pound2014factory].

## Defining Factory Physics Laws

The Flow Benchmarking analysis is grounded on Little's Law (WIP = TH * CT), and utilizes three general cases as absolute benchmarks for any real manufacturing system: The **Best Case**, the **Worst Case** and the **Practical Worst Case** .Please refer to [@hopp2008factory] and [@pound2014factory] for the rationale for these laws and equations. 

I'll define these equations as R functions:

```{r FP_Laws}

calc_w0 = function(rb, t0) {rb * t0}

ct_best = function(t0, w, w0, rb) {ifelse(w<=w0,t0,w/rb)}

th_best = function(t0, w, w0, rb) {ifelse(w<=w0,w/t0,rb)}

ct_worst = function(w,t0){w*t0}

th_worst = function(t0){1/t0}

ct_marginal = function(t0,w,rb){t0+(w-1)/rb}

th_marginal = function(w0,w,rb){rb*w/(w0+w-1)}

```

In summary, these equations provide a starting point to discuss how well a manufacturing system is doing in terms of converting inventory to Throughput. The initial analysis requires two inputs. The first input is the **Bottleneck rate (rb)**, which is the production rate (parts, orders / time) of the bottleneck (defined as the process center with the highest long-term utilization). The second parameter is the **Total Raw Processing Time (t0)**, which is the sum of the long-term average process times of each processing center. Based on these two parameters, it's possible to draw benchmarking curves for the System's Throughput and Cycle Time as a function of its Work in Process, assuming a CONWIP control system [@Spearman1990].

## Drawing Absolute Benchmarking Curves

Once I have the basic laws of manufacturing dynamics as R functions, I'll create a `benchmarck_flow` function to execute the analysis. This function accepts the `rb` and `t0` parameters and will calculate the system's Throughput and Cycle time as a function of the wip under different scenarios for benchmarking purposes.

```{r}

## Defining Cycle time and Throughput functions

benchmark_flow = function(rb, t0, step = 1, wip_mult = 5) {
  
  # First, calculate wip_crit
  w0 = calc_w0(rb = rb, t0 = t0)
  
  # Then, define WIP Range to consider:
  wip = seq.int(from = 1, to = w0 * wip_mult, by = step)
  
  # Then, calculate The Best Case Variables
  Best_Cycle_Time = ct_best(t0 = t0, w = wip, w0 = w0, rb = rb)
  Best_Throughput = th_best(t0 = t0, w = wip, w0 = w0, rb = rb)
  
  best_data = data.frame(WIP = wip,
                    Throughput = Best_Throughput,
                    CycleTime = Best_Cycle_Time,
                    Scenario = "Best Case")
  
  # Calculate the Marginal Cases:
  Marginal_Cycle_Time = ct_marginal(t0=t0,w=wip,rb=rb)
  Marginal_Throughput = th_marginal(w0=w0,w=wip,rb=rb)
  
  marginal_data = data.frame(WIP = wip,
                    Throughput = Marginal_Throughput,
                    CycleTime = Marginal_Cycle_Time,
                    Scenario = "Marginal")
  
  # Calculate Worst Case
  worst_data = data.frame(
    WIP = wip,
    Throughput = th_worst(t0 = t0),
    CycleTime = ct_worst(w = wip, t0 = t0),
    Scenario = "Worst Case"
  )

  # Output A DataFrame with results:
  # I'm not including the Worst Case because it's unrealistic (and messes up my cycle time plot).
  rbind(best_data, marginal_data, worst_data)
  
}

# The First Penny Fab Example:
data_benchmark = benchmark_flow(rb = 0.5, t0 = 8)

knitr::kable(head(data_benchmark))
```


# How would the Actual System Behave?

Ok, now I have a table with all the basic benchmarking results. What if I have a better model of the system? We can accomplish this by building a discrete event simulation model of the actual system, and using a [metamodel](/post/post/des-metamodeling-splines-r-arena/) of this model approximate its results. During my course, I used several different [Arena Simulation](https://www.arenasimulation.com) Models to illustrate that adding variability to the system always degrades performance (as the theory predicts!). Doing so allowed the students to build confidence into the model and the theory, which was great! (regret not doing so previously in simulation projects!).

```{r simulation-model}
library(arena2r)
library(tidyr)
library(splines)

arena_data = arena2r::get_simulation_results("2019-data/penny-fab")

# Filtering only Statistics of our Interest:

filtered_data = subset(arena_data, Statistic %in% c("w", "LeadTime", "Throughput"))

# Spreading and Data Wrangling

final_data = filtered_data %>% 
  tidyr::spread(Statistic, Value) %>%
  dplyr::select(LeadTime, Throughput, w)

colnames(final_data) = c("CycleTime", "Throughput", "WIP")

# Now, build a spline metamodel for CycleTime and Throughput as a function of WIP.

th_model = lm(Throughput ~ splines::bs(WIP), data = final_data)

ct_model = lm(CycleTime ~ WIP, data = final_data)

# Put Together a Final DataFrame like the Benchmarking:

model_data = data.frame(
  WIP = unique(data_benchmark$WIP),
  Throughput = predict(th_model, subset(data_benchmark, Scenario == "Best Case")),
  CycleTime = predict(ct_model, subset(data_benchmark, Scenario == "Best Case")),
  Scenario = "DES Model"
)

# Adding our Model's Data to the DataFrame:

data_benchmark = rbind(
  data_benchmark,
  model_data
)


```

Ok, so Now, let's plot it!

```{r factory-physics-flow-benchmarking-cycletime-wip-throughput-plot}
library(tidyr)
library(ggplot2)
library(viridis)

# Lets define a wrapper function for our plot:

plot_benchmarking = function(data) {
  data %>%
    gather(-WIP, -Scenario, key = "var", value = "Value") %>%
  ggplot(aes(x = WIP, y = Value, color = Scenario)) +
    geom_line(size = 1) +
    facet_wrap(~ var, scales = "free", nrow = 2, ncol = 1) +
    labs(title = "Flow Benchmarking Plot") +
    scale_color_viridis(discrete = TRUE, option = "D") + 
    theme_bw()
}

# Then let's just benchmark and plot!

plot_benchmarking(data = data_benchmark)
  

```

Cool! My simulation metamodel is still quite equivalent to the marginal case. 

However, it has one advantage. I can now build a model that can simulate arbitrarily complex scenarios (e,g.: I can include different product routings, change product mix, include non-stationary demand, simulate setup time reduction, even maybe use a multi-method model, etc.) and I can still use my model will be likely a better approximation of the actual system than any simple queueing network model.

## Wrapping up with Tradeoffs and Many-Objective Visuals

I also used simulation models to illustrate the impacts of not optimizing inventory systems, or optimizing them with an arbitrary service level definition. Unfortunetly, trying to use R to this task wasn't productive. I ended up using [DiscoveryDV](https://www.decisionvis.com/ddv/), which is a great tool for many-objective visualization.


![Tradeoffs in Inventory Management](/post/2019-data/many-objective-inventory-tradeoffs.gif) 



```{r, echo=FALSE, eval = FALSE}

plot_tradeoff = function(data) {
    # Worst case is too pessimistic!
    subset(data[,2:4], Scenario != "Worst Case") %>%
    # gather(-Scenario, key = "var", value = "Value") %>%
  ggplot(aes(x = Throughput,y = CycleTime, color = Scenario)) +
    geom_line(size = 1) +
    labs(title = "Tradeoff Frontier Plot") +
    scale_color_viridis(discrete = TRUE, option = "D") + 
    theme_bw()
}

plot_tradeoff(data = data_benchmark)

```

## Final Thoughts

After plotting the benchmarking curves, compare the curves with the system's actual condition (mark an "x" indicating the system's actual WIP, Cycle Time and Throughput), and have a productive discussion about how to improve it. 

This discussion might lead to several questions like these: "What if we change our batch sizes? What if we were able to invest on a more reliable machine? What if...?" These questions may lead to two possible outcomes. One possible direction is finding obvious quick wins. In this case, there's no need to further discussion or analysis. However, it's also possible to find high-cost and high-potential ideas. In this case, it might be a good idea to use the tool for the job: simulation! In that case, it might be useful to improve the underlying simulation model allowing it to simulate the potential high-cost improvements.



Again, I suggest you to read the [@pound2014factory] book to understand the many uses of this plot.

As I demonstrated, using a metamodel is interesting because you can have more confidence that the system will behave as expected *if* you decide to reduce or increase WIP, for instance. In this particular case, the model behavior roughly maches the Marginal Case behavior, but this may not be the case in the real system. In case the system currently performs worse than the Marginal Case, the model can be used to simulate different improvement scenarios, and how they push the system towards the benchmark curves. Doing this would allow you to gain predictive insight of where the system **will be** if you add WIP or make improvements.



# References



