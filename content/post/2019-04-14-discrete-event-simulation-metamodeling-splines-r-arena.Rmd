---
title: Discrete Event Simulation (DES) Metamodeling - Splines with R and Arena
author: Pedro N. de Lima
date: '2019-04-14'
slug: des-metamodeling-splines-r-arena
categories:
  - R Blogs
  - R
tags:
  - Arena Simualtion
  - Arena2R
  - Discrete Event Simulation
  - Metamodeling
  - R
header:
  caption: ''
  image: '/thumbnails/splines-post.png'
bibliography: [references/references-metamodeling.bib]
link-citations: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


[Simulation Metamodeling](https://link.springer.com/referenceworkentry/10.1007%2F978-1-4419-1153-7_957) - building and using surrogate models that can approximate results from more complicated simulation models - is an interesting approach to analyze results from complicated, computationally expensive simulation models. Metamodels are useful because they can yield good approximations of the original simulation model response variables using less computational resources. For an introduction to Metamodeling, refer to [@Barton2015]. 

To my knowledge, no Discrete-Event Simulation (DES) software provides metamodeling capabilities, and guidance on how to actually execute metamodeling [is scarce](https://www.google.com/search?q=discrete+event+simulation+metamodeling&oq=discrete+event+simulation+metamodeling). In this post, I'll build a [Spline](https://en.wikipedia.org/wiki/Spline_(mathematics))-based simulation metamodel. This tutorial should be useful to advanced users of [Arena Simulation](https://www.arenasimulation.com) who would be willing to give metamodeling a try.

### Why Splines?

In my [previous post](/post/making-sense-of-models-with-metamodels-low-order-polynomialss-with-arena-and-r/), I briefly described the motivation for using metamodels to approximate simulation models results. Splines are among the useful techniques for metamodeling because: (i) they are relatively simple (they are piecewise-defined polynomials), and (ii) Unlike low-order polynomials, you can generally use them with a global sampling strategy [@Barton2006], meaning you can just sample a wide range of input values of your control variable and your model will still have a decent fit.

### Data Wrangling

Before developing our metamodel, let's first load the simulation data and do some data wrangling. For the details on this step, please refer to my [previous post](/post/making-sense-of-models-with-metamodels-low-order-polynomialss-with-arena-and-r/).

```{r}
library(arena2r)
library(dplyr)
library(ggplot2)
library(readr)

sim_results = arena2r::get_simulation_results(source = "2019-03-metamodeling/")

sim_results$BatchSize = readr::parse_number(as.character(sim_results$Scenario))

sim_results = subset(sim_results, Statistic == "Entity 1.NumberOut")

head(sim_results)

```


### Trying Splines

You can build a spline model with the R's standard linear model `lm` function. Instead of using the standard `Y ~ X` formula, we just have to use the `bs()` function from the `splines` package. Thus, our formula for our spline metamodel will be `Y ~ bs(X)`.

```{r splines}
## Now using Splines:

library(splines)

# Building a Spline Model:

spline_model <-lm(Value ~ bs(BatchSize),data = sim_results)

summary(spline_model)

```

Once you have your `spline_model`, you can use the `predict` function to estimate the expected value of the response variable. Estimating what will be the Expected value of the Output variable with a Batch Size of 200 units is easy as:

```{r}
predict(spline_model, 
        newdata = data.frame(BatchSize = 200))
```

### "Optimizing" with Splines

Now that we have a spline model that can approximate our model results, we will use this model to find an "optimal" Batch Size which maximizes our Output Variable.

```{r}
## Defining limits:
batchlims <- range(sim_results$BatchSize)

# Generating Test Data
batch.grid<-seq(from=batchlims[1], to = batchlims[2])

# Using the metamodel:
spline_data = data.frame(BatchSize = batch.grid, 
                         Value = predict(spline_model,
                                         newdata = list(BatchSize=batch.grid))
                         )

# What is the Batch Size which "optimizes" the Output?
Optimum_BatchSize <- spline_data$BatchSize[which.max(spline_data$Value)]

Output_Value <- spline_data$Value[which.max(spline_data$Value)]

Optimum_BatchSize

```

The suggested batch size is `r Optimum_BatchSize`. Is this a reasonable guess, based on our simulation runs? Let's figure this out by plotting the simulation data, the spline function and the optimum value found.

```{r}

# Let's plot again with the optimum batch size:
ggplot(sim_results, mapping = aes(x = BatchSize, y = Value)) + 
  geom_point() + 
  stat_smooth(method = lm, formula = y ~ splines::bs(x)) +
  geom_vline(xintercept = Optimum_BatchSize) + 
  geom_text(aes(x=Optimum_BatchSize, 
                label="\nOptimum Batch Size", y=9700), 
                angle=90, 
                text=element_text(size=11)
            ) + 
  labs(y = "Output")


```

Yes, definitely this is a good estimate! This plot encourages one to avoid going below 300 units, and suggests that going 350 and above is not a good idea either. The interesting pattern that the spline curve suggests is that increasing Batchsize not always increases Output, and that the output loss is not symetric. 

Acknowledging these non-linear relationships is one of the outcomes I value at the end of a simulation project, and I hope that metamodeling will be an useful tool to you as well. Splines are a straightforward option to interpolate results from a simulation model, but there are other options out there. Future posts might explore other alternatives such as kriging metamodels, neural nets, and other techniques.


# References


