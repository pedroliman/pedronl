<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R Blogs by Pedro N. de Lima</title>
    <link>/categories/r-blogs/</link>
    <description>Recent content in R Blogs on Pedro N. de Lima</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Mon, 22 Apr 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/categories/r-blogs/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Factory Physics Flow Benchmarking in R</title>
      <link>/post/factory-physics-flow-benchmarking-in-r/</link>
      <pubDate>Mon, 22 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/factory-physics-flow-benchmarking-in-r/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://factoryphysics.com/flow-benchmarking&#34;&gt;Factory Physics (FP) Flow Benchmarking&lt;/a&gt; is a technique useful to perform absolute benchmark of a system’s throughput and cycle time, given the Work in Process (wip) the system is allowed to have. These techniques are introduced in &lt;span class=&#34;citation&#34;&gt;(Pound, Bell, and Spearman &lt;a href=&#34;#ref-pound2014factory&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(Hopp and Spearman &lt;a href=&#34;#ref-hopp2008factory&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;, and repeating this discussion is beyond the scope of this post.&lt;/p&gt;
&lt;p&gt;As a simulation pratictioneer and lecturer, as I read these books, I naturally started creating models and spreadsheet formulas to verify if the theory holds (and, of course, it does). After reading the book, I found my self wondering: “How do I teach these things? If I were to teach my students to perform Flow Benchmarking, what would this class require?”&lt;/p&gt;
&lt;p&gt;This post offers my answer to this question using &lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt;. Naturally, this post will be usefull to those willing to use their Simulation models to perform flow benchmarking as prescribed in &lt;span class=&#34;citation&#34;&gt;(Pound, Bell, and Spearman &lt;a href=&#34;#ref-pound2014factory&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;defining-fp-laws&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Defining FP Laws&lt;/h2&gt;
&lt;p&gt;The FP book defines a set of equations which serve as absolute benchmarks for any manufacturing systems &lt;span class=&#34;citation&#34;&gt;(Hopp and Spearman &lt;a href=&#34;#ref-hopp2008factory&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;. These equations define the maximum Throughput and minimum Cycle Times a manufacturing system can have, given its work in process (wip), bottleneck rate (rb) and the sum of the mean processing times of all stations (t0), assuming that the system is using a CONWIP system &lt;span class=&#34;citation&#34;&gt;(SPEARMAN, WOODRUFF, and HOPP &lt;a href=&#34;#ref-Spearman1990&#34;&gt;1990&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;First, then, I’ll create these equations as R functions, which will be useful later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_w0 = function(rb, t0) {rb * t0}

ct_best = function(t0, w, w0, rb) {ifelse(w&amp;lt;=w0,t0,w/rb)}

th_best = function(t0, w, w0, rb) {ifelse(w&amp;lt;=w0,w/t0,rb)}

ct_worst = function(w,t0){w*t0}

th_worst = function(t0){1/t0}

ct_marginal = function(t0,w,rb){t0+(w-1)/rb}

th_marginal = function(w0,w,rb){rb*w/(w0+w-1)}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Defining Cycle time and Throughput functions

benchmark_flow = function(rb, t0, step = 1, wip_mult = 5) {
  
  # First, calculate wip_crit
  w0 = calc_w0(rb = rb, t0 = t0)
  
  # Then, define WIP Range to consider:
  
  wip = seq.int(from = 1, to = w0 * wip_mult, by = step)
  
  # Then, calculate The Best Case Variables
  Best_Cycle_Time = ct_best(t0 = t0, w = wip, w0 = w0, rb = rb)
  Best_ThroughPut = th_best(t0 = t0, w = wip, w0 = w0, rb = rb)
  
  best_data = data.frame(WIP = wip,
                    ThroughPut = Best_ThroughPut,
                    CycleTime = Best_Cycle_Time,
                    Scenario = &amp;quot;Best&amp;quot;)
  
  # Calculate the Marginal Cases:
  Marginal_Cycle_Time = ct_marginal(t0=t0,w=wip,rb=rb)
  Marginal_ThroughPut = th_marginal(w0=w0,w=wip,rb=rb)
  
  marginal_data = data.frame(WIP = wip,
                    ThroughPut = Marginal_ThroughPut,
                    CycleTime = Marginal_Cycle_Time,
                    Scenario = &amp;quot;Marginal&amp;quot;)

  # Output A DataFrame with results:
  rbind(best_data, marginal_data)
  
}

# Perform a BenchMarch of the First Penny Fab:
data_benchmark = benchmark_flow(rb = 0.5, t0 = 8)

data_benchmark&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    WIP ThroughPut CycleTime Scenario
## 1    1  0.1250000         8     Best
## 2    2  0.2500000         8     Best
## 3    3  0.3750000         8     Best
## 4    4  0.5000000         8     Best
## 5    5  0.5000000        10     Best
## 6    6  0.5000000        12     Best
## 7    7  0.5000000        14     Best
## 8    8  0.5000000        16     Best
## 9    9  0.5000000        18     Best
## 10  10  0.5000000        20     Best
## 11  11  0.5000000        22     Best
## 12  12  0.5000000        24     Best
## 13  13  0.5000000        26     Best
## 14  14  0.5000000        28     Best
## 15  15  0.5000000        30     Best
## 16  16  0.5000000        32     Best
## 17  17  0.5000000        34     Best
## 18  18  0.5000000        36     Best
## 19  19  0.5000000        38     Best
## 20  20  0.5000000        40     Best
## 21   1  0.1250000         8 Marginal
## 22   2  0.2000000        10 Marginal
## 23   3  0.2500000        12 Marginal
## 24   4  0.2857143        14 Marginal
## 25   5  0.3125000        16 Marginal
## 26   6  0.3333333        18 Marginal
## 27   7  0.3500000        20 Marginal
## 28   8  0.3636364        22 Marginal
## 29   9  0.3750000        24 Marginal
## 30  10  0.3846154        26 Marginal
## 31  11  0.3928571        28 Marginal
## 32  12  0.4000000        30 Marginal
## 33  13  0.4062500        32 Marginal
## 34  14  0.4117647        34 Marginal
## 35  15  0.4166667        36 Marginal
## 36  16  0.4210526        38 Marginal
## 37  17  0.4250000        40 Marginal
## 38  18  0.4285714        42 Marginal
## 39  19  0.4318182        44 Marginal
## 40  20  0.4347826        46 Marginal&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then let’s plot!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
library(ggplot2)

# Lets define a wrapper function to our plot:

plot_benchmarking = function(data) {
  data %&amp;gt;%
    gather(-WIP, -Scenario, key = &amp;quot;var&amp;quot;, value = &amp;quot;Value&amp;quot;) %&amp;gt;%
  ggplot(aes(x = WIP, y = Value, color = Scenario)) +
    geom_line() +
    facet_wrap(~ var, scales = &amp;quot;free&amp;quot;) +
    labs(title = &amp;quot;FP Flow Benchmarking Plot&amp;quot;) +
    theme_bw()
}

# Then let&amp;#39;s just benchmark and plot!

plot_benchmarking(data = benchmark_flow(rb = 0.5, t0 = 8, wip_mult = 10))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-22-factory-physics-flow-benchmarking-with-arena-and-r_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;After plotting the benchmark curves, one can have productive discussion about what kinds of improvements are likely to produce good results.&lt;/p&gt;
&lt;p&gt;Now its your turn. Copy and paste the R code, and insert the rb parameter (bottleneck rate), and t0 ()&lt;/p&gt;
&lt;p&gt;The beauty of this analysis is that now you can pin point where any manufacturing system is in terms of WIP, Cycle Time and Throughput, and have a productive discussion about how to improve it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-hopp2008factory&#34;&gt;
&lt;p&gt;Hopp, W.J., and M.L. Spearman. 2008. &lt;em&gt;Factory Physics&lt;/em&gt;. Irwin/Mcgraw-Hill Series in Operations and Decision Sciences. McGraw-Hill. &lt;a href=&#34;https://books.google.com.br/books?id=tEjkAAAACAAJ&#34;&gt;https://books.google.com.br/books?id=tEjkAAAACAAJ&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pound2014factory&#34;&gt;
&lt;p&gt;Pound, E.S., J.H. Bell, and M.L. Spearman. 2014. &lt;em&gt;Factory Physics for Managers: How Leaders Improve Performance in a Post-Lean Six Sigma World&lt;/em&gt;. McGraw-Hill Education. &lt;a href=&#34;https://books.google.com.br/books?id=B5sXAwAAQBAJ&#34;&gt;https://books.google.com.br/books?id=B5sXAwAAQBAJ&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Spearman1990&#34;&gt;
&lt;p&gt;SPEARMAN, MARK L., DAVID L. WOODRUFF, and WALLACE J. HOPP. 1990. “CONWIP: A Pull Alternative to Kanban.” &lt;em&gt;International Journal of Production Research&lt;/em&gt; 28 (5). Taylor &amp;amp; Francis: 879–94. &lt;a href=&#34;https://doi.org/10.1080/00207549008942761&#34;&gt;https://doi.org/10.1080/00207549008942761&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Discrete Event Simulation (DES) Metamodeling - Splines with R and Arena</title>
      <link>/post/des-metamodeling-splines-r-arena/</link>
      <pubDate>Sun, 14 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/des-metamodeling-splines-r-arena/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://link.springer.com/referenceworkentry/10.1007%2F978-1-4419-1153-7_957&#34;&gt;Simulation Metamodeling&lt;/a&gt; - building and using surrogate models that can approximate results from more complicated simulation models - is an interesting approach to analyze results from complicated, computationally expensive simulation models. Metamodels are useful because they can yield good approximations of the original simulation model response variables using less computational resources. For an introduction to Metamodeling, refer to &lt;span class=&#34;citation&#34;&gt;(Barton &lt;a href=&#34;#ref-Barton2015&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To my knowledge, no Discrete-Event Simulation (DES) software provides metamodeling capabilities, and guidance on how to actually execute metamodeling &lt;a href=&#34;https://www.google.com/search?q=discrete+event+simulation+metamodeling&amp;amp;oq=discrete+event+simulation+metamodeling&#34;&gt;is scarce&lt;/a&gt;. In this post, I’ll build a &lt;a href=&#34;https://en.wikipedia.org/wiki/Spline_(mathematics)&#34;&gt;Spline&lt;/a&gt;-based simulation metamodel. This tutorial should be useful to advanced users of &lt;a href=&#34;https://www.arenasimulation.com&#34;&gt;Arena Simulation&lt;/a&gt; who would be willing to give metamodeling a try.&lt;/p&gt;
&lt;div id=&#34;why-splines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why Splines?&lt;/h3&gt;
&lt;p&gt;In my &lt;a href=&#34;/post/making-sense-of-models-with-metamodels-low-order-polynomialss-with-arena-and-r/&#34;&gt;previous post&lt;/a&gt;, I briefly described the motivation for using metamodels to approximate simulation models results. Splines are among the useful techniques for metamodeling because: (i) they are relatively simple (they are piecewise-defined polynomials), and (ii) Unlike low-order polynomials, you can generally use them with a global sampling strategy &lt;span class=&#34;citation&#34;&gt;(Barton and Meckesheimer &lt;a href=&#34;#ref-Barton2006&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt;, meaning you can just sample a wide range of input values of your control variable and your model will still have a decent fit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-wrangling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Wrangling&lt;/h3&gt;
&lt;p&gt;Before developing our metamodel, let’s first load the simulation data and do some data wrangling. For the details on this step, please refer to my &lt;a href=&#34;/post/making-sense-of-models-with-metamodels-low-order-polynomialss-with-arena-and-r/&#34;&gt;previous post&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(arena2r)
library(dplyr)
library(ggplot2)
library(readr)

sim_results = arena2r::get_simulation_results(source = &amp;quot;2019-03-metamodeling/&amp;quot;)

sim_results$BatchSize = readr::parse_number(as.character(sim_results$Scenario))

sim_results = subset(sim_results, Statistic == &amp;quot;Entity 1.NumberOut&amp;quot;)

head(sim_results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        Scenario          Statistic Replication Value BatchSize
## 51 BatchSize200 Entity 1.NumberOut           1  9200       200
## 52 BatchSize200 Entity 1.NumberOut           2  9368       200
## 53 BatchSize200 Entity 1.NumberOut           3  9322       200
## 54 BatchSize200 Entity 1.NumberOut           4  9039       200
## 55 BatchSize200 Entity 1.NumberOut           5  9255       200
## 56 BatchSize200 Entity 1.NumberOut           6  9400       200&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;trying-splines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Trying Splines&lt;/h3&gt;
&lt;p&gt;You can build a spline model with the R’s standard linear model &lt;code&gt;lm&lt;/code&gt; function. Instead of using the standard &lt;code&gt;Y ~ X&lt;/code&gt; formula, we just have to use the &lt;code&gt;bs()&lt;/code&gt; function from the &lt;code&gt;splines&lt;/code&gt; package. Thus, our formula for our spline metamodel will be &lt;code&gt;Y ~ bs(X)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Now using Splines:

library(splines)

# Building a Spline Model:

spline_model &amp;lt;-lm(Value ~ bs(BatchSize),data = sim_results)

summary(spline_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Value ~ bs(BatchSize), data = sim_results)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -221.646  -30.896    1.011   46.969  268.354 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)     9256.49      30.15  306.99  &amp;lt; 2e-16 ***
## bs(BatchSize)1  1312.17     102.27   12.83  &amp;lt; 2e-16 ***
## bs(BatchSize)2  1042.28      88.29   11.80 1.61e-15 ***
## bs(BatchSize)3   961.00      42.95   22.38  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 96.04 on 46 degrees of freedom
## Multiple R-squared:  0.9465, Adjusted R-squared:  0.943 
## F-statistic: 271.1 on 3 and 46 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once you have your &lt;code&gt;spline_model&lt;/code&gt;, you can use the &lt;code&gt;predict&lt;/code&gt; function to estimate the expected value of the response variable. Estimating what will be the Expected value of the Output variable with a Batch Size of 200 units is easy as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(spline_model, 
        newdata = data.frame(BatchSize = 200))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        1 
## 9256.489&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;optimizing-with-splines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;“Optimizing” with Splines&lt;/h3&gt;
&lt;p&gt;Now that we have a spline model that can approximate our model results, we will use this model to find an “optimal” Batch Size which maximizes our Output Variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Defining limits:
batchlims &amp;lt;- range(sim_results$BatchSize)

# Generating Test Data
batch.grid&amp;lt;-seq(from=batchlims[1], to = batchlims[2])

# Using the metamodel:
spline_data = data.frame(BatchSize = batch.grid, 
                         Value = predict(spline_model,
                                         newdata = list(BatchSize=batch.grid))
                         )

# What is the Batch Size which &amp;quot;optimizes&amp;quot; the Output?
Optimum_BatchSize &amp;lt;- spline_data$BatchSize[which.max(spline_data$Value)]

Output_Value &amp;lt;- spline_data$Value[which.max(spline_data$Value)]

Optimum_BatchSize&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 331&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The suggested batch size is 331. Is this a reasonable guess, based on our simulation runs? Let’s figure this out by plotting the simulation data, the spline function and the optimum value found.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Let&amp;#39;s plot again with the optimum batch size:
ggplot(sim_results, mapping = aes(x = BatchSize, y = Value)) + 
  geom_point() + 
  stat_smooth(method = lm, formula = y ~ splines::bs(x)) +
  geom_vline(xintercept = Optimum_BatchSize) + 
  geom_text(aes(x=Optimum_BatchSize, 
                label=&amp;quot;\nOptimum Batch Size&amp;quot;, y=9700), 
                angle=90, 
                text=element_text(size=11)
            ) + 
  labs(y = &amp;quot;Output&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-14-discrete-event-simulation-metamodeling-splines-r-arena_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yes, definitely this is a good estimate! This plot encourages one to avoid going below 300 units, and suggests that going 350 and above is not a good idea either. The interesting pattern that the spline curve suggests is that increasing Batchsize not always increases Output, and that the output loss is not symetric.&lt;/p&gt;
&lt;p&gt;Acknowledging these non-linear relationships is one of the outcomes I value at the end of a simulation project, and I hope that metamodeling will be an useful tool to you as well. Splines are a straightforward option to interpolate results from a simulation model, but there are other options out there. Future posts might explore other alternatives such as kriging metamodels, neural nets, and other techniques.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Barton2006&#34;&gt;
&lt;p&gt;Barton, Russell R, and Martin Meckesheimer. 2006. “Metamodel-Based Simulation Optimization” 13 (06). &lt;a href=&#34;https://doi.org/10.1016/S0927-0507(06)13018-2&#34;&gt;https://doi.org/10.1016/S0927-0507(06)13018-2&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Barton2015&#34;&gt;
&lt;p&gt;Barton, Russel R. 2015. “Tutorial: Simulation Metamodeling.” In &lt;em&gt;Proceedings of the 2015 Winter Simulation Conference&lt;/em&gt;, 1765–79.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Making Sense of Simulation Models with Metamodels Part 1 - Low-Order Polynomials with Arena and R</title>
      <link>/post/making-sense-of-models-with-metamodels-low-order-polynomialss-with-arena-and-r/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/making-sense-of-models-with-metamodels-low-order-polynomialss-with-arena-and-r/</guid>
      <description>&lt;p&gt;This is part 1 of a series of posts in which I will explore the utility of using metamodels to make sense of (and possibly optimizing) simulation models.&lt;/p&gt;
&lt;p&gt;If you used simulation modeling on a real project, you might be familiar with this fictional story:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You spent long hours building and refining your simulation model (eg.: a Discrete Event Model). Hopefully, you are confident that it can yield reliable results. Now it’s time to use the model and draw recommendations. At this point, you are probably out of time, the project was delayed by successive rounds of data collection and validation. After running a few scenarios the night before the final presentation, you reach the conclusion that it is going to be hard to explain to your client that the results are highly non-linear and maybe counter-intuitive.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;making-sense-and-possibly-optimizing-models-with-metamodels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Making Sense (and possibly optimizing) Models with Metamodels&lt;/h2&gt;
&lt;p&gt;The idea of building a simple model as a surrogate of a more complicated model might seem analytical overkill. However, long ago, scholars have recognized the utility of using more explicit models to synthesize simulation results, and to find optimal parameters for models with long run time. Refer to &lt;span class=&#34;citation&#34;&gt;(Kleijnen &lt;a href=&#34;#ref-Kleijnen2017&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(Barton and Meckesheimer &lt;a href=&#34;#ref-Barton2006&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt; for comprehensive reviews on Metamodeling for optimization.&lt;/p&gt;
&lt;p&gt;In this post, I will show you how to analyze an Arena Discrete Event Model in R using Low-Order Polynomials.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example-with-arena-and-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Example with Arena and R&lt;/h2&gt;
&lt;p&gt;In this example, the goal is to find an “ideal” batch size, so that our expected output is maximized. Setting the Batch Size “too low”, causes the production system to lose too much time in setups (a setup is required for every batch). Setting the batch size “too high” can cause starvation in other job stations. What “too low” or “too high” means is dependent on various factors, such as cycle times, setup times and other model parameters. Also, improvements in the system may cause the “ideal” batch size to change, but we can’t figure this out without a model.&lt;/p&gt;
&lt;p&gt;Although this example is simple, the underlying idea can be generalized to any case in which a response variable is concave (e.g., Total Costs, Revenue, Throughput) in respect to a decision variable, and your goal is to figure out what this relationship looks like to better manage the system.&lt;/p&gt;
&lt;p&gt;After this introduction, we are going to focus on how to create a metamodel after simulating a few scenarios with Arena.&lt;/p&gt;
&lt;div id=&#34;data-wrangling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Wrangling&lt;/h3&gt;
&lt;p&gt;The first step is obtaining a data.frame where individual observations are simulation replications, and we have one column as the dependent variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and another column as the independent variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, so that we can find a function &lt;span class=&#34;math inline&#34;&gt;\(y = f_{meta}(x)\)&lt;/span&gt; that will provide an aproximation of our model results. This aproximation should be usefull to explain the relationship between &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;First, I simulated all scenarios and saved their results as separate csv files. You can download the files &lt;a href=&#34;/post/2019-03-metamodeling/batchsizefiles.zip&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As you can see opening these files, Arena’s output files need work to become a useful tidy dataframe. By using the package &lt;a href=&#34;arena2r.pedronl.com&#34;&gt;Arena2R&lt;/a&gt;, I can obtain my dataframe easily with the function ‘get_simulation_results’, which will read all csv files in a given path and provide a tidy data.frame with all simulation results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(arena2r)
library(dplyr)
library(ggplot2)
library(readr)

# Obtaining a dataframe compiling all simulation results stored at the &amp;quot;source&amp;quot; folder.
sim_results = arena2r::get_simulation_results(source = &amp;quot;2019-03-metamodeling/&amp;quot;)

head(sim_results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Scenario                   Statistic Replication    Value
## 1 BatchSize200 Colagem.Queue.NumberInQueue           1 9.476321
## 2 BatchSize200 Colagem.Queue.NumberInQueue           2 7.313429
## 3 BatchSize200 Colagem.Queue.NumberInQueue           3 8.647507
## 4 BatchSize200 Colagem.Queue.NumberInQueue           4 7.966887
## 5 BatchSize200 Colagem.Queue.NumberInQueue           5 9.214783
## 6 BatchSize200 Colagem.Queue.NumberInQueue           6 8.143359&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although this dataframe is a good starting point, it does not contain our independent variable (the Batch Size) as a numeric value. I coded my output files so that they will always correspond to BatchSizeXXX, wherein XXX will be a number. After some data wrangling we will be good to continue our metamodeling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Creating a Column For The Dependent Variable, assigning it to the number in the file name:
sim_results$BatchSize = readr::parse_number(as.character(sim_results$Scenario))

# Filter only the Outcome Variable of Interest

sim_results = subset(sim_results, Statistic == &amp;quot;Entity 1.NumberOut&amp;quot;)

# Now Let&amp;#39;s view the relationship between BatchSize and Throughput:

ggplot(sim_results, mapping = aes(x = BatchSize, y = Value, color = Value)) + 
  geom_point() +
  labs(y = &amp;quot;Output&amp;quot;, color = &amp;quot;Output&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-09-making-sense-of-models-with-metamodels-low-order-polynomials-with-arena-and-r_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot shows us important lessons about &lt;em&gt;non-linearity&lt;/em&gt;. Clearly, the output variable has a non-linear relationship with Batch Size. The tricky implication is that if you decided to sample only values of BatchSize &amp;gt; 300, you might reach the conclusion that increasing Batch Size has little impact on Output, and this impact is likely negative. Conversely, if you sample only BatchSize &amp;lt; 300, you would reach the opposite conclusion.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;drawing-curves-revealing-non-linear-patterns&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Drawing Curves, Revealing Non-Linear Patterns&lt;/h2&gt;
&lt;p&gt;If you could draw a curve explaining the relationship between BatchSize and the Output Variable, what would this curve look like? That’s where polynomial metamodels in.&lt;/p&gt;
&lt;p&gt;You can find documentation about polynomial regression in R &lt;a href=&#34;https://www.r-bloggers.com/fitting-polynomial-regression-in-r/&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://medium.com/wwblog/polynomial-regression-in-r-c377f18d6efa&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://www.theanalysisfactor.com/r-tutorial-4/&#34;&gt;here&lt;/a&gt;. Put simply, regression modeling can be seen as drawing lines (or maybe curves) with the purpose of revealing the existence of relationships between variables. In our case, we will first use a polynomial function in the form $y = a + bx + cx^2 $ that will be useful to picture the non-linear relationship between BatchSize and the Output Variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(sim_results, mapping = aes(x = BatchSize, y = Value)) + 
  geom_point() + 
  stat_smooth(method = &amp;quot;lm&amp;quot;, formula = y ~ x + I(x^2), size = 1) + 
  labs(y = &amp;quot;Output&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-09-making-sense-of-models-with-metamodels-low-order-polynomials-with-arena-and-r_files/figure-html/second-order-polynomial-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;optimizing-with-a-quadratic-metamodel&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;“Optimizing” with a Quadratic Metamodel&lt;/h3&gt;
&lt;p&gt;Since our model is quadratic, we can do some calculus to find the point in which the output peaks:&lt;/p&gt;
&lt;p&gt;Since our function is clearly concave down, we can use simple calculus to find the BatchSize Value that maximizes the Output:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_{opt} = \arg\max \ \  ax^2 + bx + c \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can find the optimal point by taking the first derivative:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y&amp;#39; = 2ax + b\]&lt;/span&gt;
Since we know our model is concave down, we know that when the first derivative reaches 0, we will be at its maximum value.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[0 = 2ax_{opt} + b\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_{opt} = -b / 2a\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now that we have a formula, let’s calculate the optimum batch size (based on our metamodel):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quadratic_model = lm(formula = Value ~ BatchSize + I(BatchSize^2), data = sim_results)

## Since our Model is Quadratic, we can derive a formula for the maximum, based on our results

Optimum_BatchSize = - quadratic_model$coefficients[2] / (2 * quadratic_model$coefficients[3])

Optimum_BatchSize&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## BatchSize 
##  342.8003&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does it make sense?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(sim_results, mapping = aes(x = BatchSize, y = Value)) + 
  geom_point() + 
  stat_smooth(method = &amp;quot;lm&amp;quot;, formula = y ~ x + I(x^2), size = 1) +
  geom_vline(xintercept = Optimum_BatchSize) + 
  geom_text(aes(x=Optimum_BatchSize, label=&amp;quot;\nOptimum Batch Size&amp;quot;, y=9700), angle=90, text=element_text(size=11)) + 
  labs(y = &amp;quot;Output&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-09-making-sense-of-models-with-metamodels-low-order-polynomials-with-arena-and-r_files/figure-html/second-order-polynomial-with-optimum-value-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;caveats&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Caveats&lt;/h3&gt;
&lt;p&gt;There are a few caveats you should be aware of when using polynomial metamodels, and I’m citing only two of them here:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Use only low-order polynomials.&lt;/strong&gt; First, if you try a higher order polynomial (for instance, one that includes &lt;span class=&#34;math inline&#34;&gt;\(x^5\)&lt;/span&gt;), you will likely end up with an overfitted model. Try that and see that for yourself.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Avoid using them “Globally”&lt;/strong&gt;: You should avoid using low-order polynomials globally simply because they will become inacurate as you expand the sampling space. Look at the figure above. When Batch Size = 350, the quadratic model over-estimates the Output. There’s a workaround this called “Splines” which will be explored on another post, and there are better options (such as Kriging / Gaussian processes, Neural Nets, etc.).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Using low-order polynomials is a relatively straightforward option you can use to explore and visualize non-linear relationships between decision variables and outcome variables. However, simple polynomial models are limited, and more advanced techniques are available (including Splines, Gaussian Processes, and Neural Nets). The good news is that you can easily find documentation about these techniques in R. Once you have the data, putting together a metamodel in R is usually only a few keystrokes away. In future posts, I will continue to explore increasingly complex metamodels, but keep in mind that the goal should not be to add complexity to the analysis “just because we can”, but to add interpretability and meaning to our results.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Barton2006&#34;&gt;
&lt;p&gt;Barton, Russell R, and Martin Meckesheimer. 2006. “Metamodel-Based Simulation Optimization” 13 (06). &lt;a href=&#34;https://doi.org/10.1016/S0927-0507(06)13018-2&#34;&gt;https://doi.org/10.1016/S0927-0507(06)13018-2&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Kleijnen2017&#34;&gt;
&lt;p&gt;Kleijnen, Jack P C. 2017. “Regression and Kriging metamodels with their experimental designs in simulation : A review.” &lt;em&gt;European Journal of Operational Research&lt;/em&gt; 256 (1). Elsevier B.V.: 1–16. &lt;a href=&#34;https://doi.org/10.1016/j.ejor.2016.06.041&#34;&gt;https://doi.org/10.1016/j.ejor.2016.06.041&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Arena2R - An R Package for Arena Simulation Users</title>
      <link>/post/arena2r-package-tutorial/</link>
      <pubDate>Thu, 27 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/arena2r-package-tutorial/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arenasimulation.com&#34;&gt;Arena Simulation&lt;/a&gt; is a well-known Discrete Event Simulation Software. However, if you are a power user you might want to extend your analysis beyond what Arena’s Process Analyzer offers. In this tutorial, I’ll guide you through the main functions of Arena2R package.&lt;/p&gt;
&lt;p&gt;If you’re not an R user, fear not! Arena2R comes with an app you can use to explore your Arena Simulation data. All you’ll have to do is to Install R and R Studio, and run two commands in your R console.&lt;/p&gt;
&lt;div id=&#34;installation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;You can install arena2r from CRAN with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;arena2r&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, load the package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(arena2r)
## Warning: package &amp;#39;arena2r&amp;#39; was built under R version 3.5.2
library(dplyr)
## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 3.5.2
## 
## Attaching package: &amp;#39;dplyr&amp;#39;
## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag
## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exporting-arena-report-database&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exporting Arena Report Database&lt;/h2&gt;
&lt;p&gt;This is a basic example which shows you how to get your Arena results quickly into R. The basic idea is to run different scenarios and save each of them to a separate csv file. (Yes, you could use Process Analyzer (PAN) to run all scenarios, but to my knowledge, there’s no way to get your data out of the PAN easily).&lt;/p&gt;
&lt;p&gt;Follow these steps to get Arena simulation results to R:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run your model with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; replications. Do not change the number of replications between scenarios.&lt;/li&gt;
&lt;li&gt;For each scenario, save a csv with simulation results clicking on “Tools &amp;gt; ReportDatabase &amp;gt; Export Summary Statistics to CSV File”. Use the standard options. If Arena throws an error, then you’ll have to figure out how to get your results into a csv file. Sometimes it’s necessary to save the report database as a *.mdb file before generating the csv file.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-shiny-app&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using the Shiny App&lt;/h2&gt;
&lt;p&gt;If you’re not familiar to R, you can run this command on R Console and use the example app.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
runArenaApp()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After running this command, the app screen will pop up. You can upload your csv files and play around with the Confidence Interval and Scatter Plots.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-package-with-an-r-script&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using the Package with an R Script&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Open a new .R file, and run the following code:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load the library:

library(arena2r)

# Define the path to your folder with Arena csv files. In my case, it&amp;#39;s here:

my_path = &amp;quot;../../../arena2r/inst/Arena14/&amp;quot;

# Then, get a tidy results data.frame out of your files!
results = arena2r::get_simulation_results(my_path)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also play around with the arena_results dataset included in the package. To use it, follow these steps:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
library(arena2r)

# Load the example dataset:
data(&amp;quot;arena_results&amp;quot;)

# Let&amp;#39;s call it results
results = arena_results

knitr::kable(head(results))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Scenario&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Replication&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NumberIn&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;233&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NumberIn&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;247&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NumberIn&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;239&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NumberIn&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;261&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NumberIn&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;264&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NumberIn&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;266&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;After these steps, now you have a tidy data.frame with your results. Let’s get into possible visualizations. Usually, you’ll be interested in the mean confidence interval for some response variable, across scenarios.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# Plot a Statistic confidence interval across scenarios for a response variable.

arena2r::plot_confint(sim_results = results, response_variable = &amp;quot;Entity 1.NumberOut&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-27-arena2r-package-tutorial_files/figure-html/arena2r-confidence-interval-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now let’s explore the relationship between two variables, across scenarios and replications:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# Now let&amp;#39;s plot analyse the relationship between two variables:

arena2r::plot_scatter(sim_results = results, x_variable = &amp;quot;Entity 1.NumberIn&amp;quot;, y_variable = &amp;quot;Entity 1.NumberOut&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-27-arena2r-package-tutorial_files/figure-html/arena2r-scatter-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now let’s go a bit deeper and leverage ggplot2 to create a plot faceted by Scenario:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# If you use ggplot and you want to get more customized plots, I suggest you to spread your data.frame:

wide_results = results %&amp;gt;%
    tidyr::spread(Statistic, Value)

# Recreating my plot with ggplot, now loking at Resource Utilization:

p = ggplot(data = wide_results, mapping = aes(x = `Resource 1.Utilization`, y = `Entity 1.NumberOut`, color = Scenario)) + geom_point() + facet_wrap(~Scenario)

p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-27-arena2r-package-tutorial_files/figure-html/arena2r-custom-ggplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, let’s summarise every statistic across all scenarios.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
statistics_summary = arena2r::get_statistics_summary(sim_results = results, confidence = 0.95)

knitr::kable(head(statistics_summary[,1:6]))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Scenario&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SD&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Min&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Max&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NumberIn&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;241.03333&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15.773140&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;209.000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;276.0000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NumberOut&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;225.13333&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.735870&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;205.000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;240.0000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NVATime&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.OtherTime&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.TotalTime&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.15272&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.850762&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.161059&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25.2438&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.TranTime&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I hope you enjoyed the package. Feel free to suggest new features and to contribute to its development!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>