<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R Blogs by Pedro N. de Lima</title>
    <link>/categories/r-blogs/</link>
    <description>Recent content in R Blogs on Pedro N. de Lima</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Mon, 01 Apr 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/categories/r-blogs/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Making Sense of Simulation Models with Metamodels Part 2 - Splines with Arena and R</title>
      <link>/post/making-sense-of-simulation-models-with-metamodels-part-2-splines-with-arena-and-r/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/making-sense-of-simulation-models-with-metamodels-part-2-splines-with-arena-and-r/</guid>
      <description>&lt;p&gt;In my &lt;a href=&#34;post/making-sense-of-models-with-metamodels-low-order-polynomialss-with-arena-and-r/&#34;&gt;last post&lt;/a&gt;, I used low-order &lt;a href=&#34;https://en.wikipedia.org/wiki/Polynomial_regression&#34;&gt;polynomial regression&lt;/a&gt; to make sense of non-linear simulation model results. In this post, I’ll demonstrate how you can use &lt;a href=&#34;https://en.wikipedia.org/wiki/Spline_(mathematics)&#34;&gt;splines&lt;/a&gt; to summarise results from an &lt;a href=&#34;https://www.arenasimulation.com/&#34;&gt;Arena&lt;/a&gt; simulation model.&lt;/p&gt;
&lt;div id=&#34;why-splines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why Splines?&lt;/h3&gt;
&lt;p&gt;In the &lt;a href=&#34;post/making-sense-of-models-with-metamodels-low-order-polynomialss-with-arena-and-r/&#34;&gt;previous post&lt;/a&gt;, I briefly described the motivation for using metamodels to approximate simulation models results. Splines are among the useful techinques for metamodeling because: (i) they are relatively simple (they are piecewise-defined polynomials); (ii) Unlike low-order polynomials, you can generally use them with a global sampling strategy &lt;span class=&#34;citation&#34;&gt;(Barton and Meckesheimer &lt;a href=&#34;#ref-Barton2006&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-wrangling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Wrangling&lt;/h3&gt;
&lt;p&gt;Before developing our metamodel, let’s first load the simulation data and do some data wrangling. For the details on this step, please refer to my &lt;a href=&#34;post/making-sense-of-models-with-metamodels-low-order-polynomialss-with-arena-and-r/&#34;&gt;previous post&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(arena2r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;arena2r&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 3.5.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dplyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(readr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;readr&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_results = arena2r::get_simulation_results(source = &amp;quot;2019-03-metamodeling/&amp;quot;)

sim_results$BatchSize = readr::parse_number(as.character(sim_results$Scenario))

sim_results = subset(sim_results, Statistic == &amp;quot;Entity 1.NumberOut&amp;quot;)

head(sim_results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        Scenario          Statistic Replication Value BatchSize
## 51 BatchSize200 Entity 1.NumberOut           1  9200       200
## 52 BatchSize200 Entity 1.NumberOut           2  9368       200
## 53 BatchSize200 Entity 1.NumberOut           3  9322       200
## 54 BatchSize200 Entity 1.NumberOut           4  9039       200
## 55 BatchSize200 Entity 1.NumberOut           5  9255       200
## 56 BatchSize200 Entity 1.NumberOut           6  9400       200&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;trying-splines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Trying Splines&lt;/h3&gt;
&lt;p&gt;The function bs from the splines package can be used for creating a b-spline term in our regression model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Now using Splines:

library(splines)

# Building a Spline Model:

spline_model &amp;lt;-lm(Value ~ bs(BatchSize),data = sim_results)

summary(spline_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Value ~ bs(BatchSize), data = sim_results)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -221.646  -30.896    1.011   46.969  268.354 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)     9256.49      30.15  306.99  &amp;lt; 2e-16 ***
## bs(BatchSize)1  1312.17     102.27   12.83  &amp;lt; 2e-16 ***
## bs(BatchSize)2  1042.28      88.29   11.80 1.61e-15 ***
## bs(BatchSize)3   961.00      42.95   22.38  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 96.04 on 46 degrees of freedom
## Multiple R-squared:  0.9465, Adjusted R-squared:  0.943 
## F-statistic: 271.1 on 3 and 46 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does it make sense?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Does it make sense ?

ggplot(sim_results, mapping = aes(x = BatchSize, y = Value)) + 
  geom_point() + 
  stat_smooth(method = lm, formula = y ~ splines::bs(x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-01-making-sense-of-simulation-models-with-metamodels-part-2-splines-with-arena-and-r_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;optimizing-with-splines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;“Optimizing” with Splines&lt;/h3&gt;
&lt;p&gt;Now that we found a curve that can approximate our model results, we will use this model to find an “optimal” Batch Size which maximizes our Output Variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Defining limits:
batchlims &amp;lt;- range(sim_results$BatchSize)

# Generating Test Data
batch.grid&amp;lt;-seq(from=batchlims[1], to = batchlims[2])

# Using the metamodel:
spline_data = data.frame(BatchSize = batch.grid, 
                         Value = predict(spline_model,
                                         newdata = list(BatchSize=batch.grid))
                         )

# What is the Batch Size which &amp;quot;optimizes&amp;quot; the Output?
Optimum_BatchSize &amp;lt;- spline_data$BatchSize[which.max(spline_data$Value)]

Output_Value &amp;lt;- spline_data$Value[which.max(spline_data$Value)]

Optimum_BatchSize&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 331&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does it make sense?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Let&amp;#39;s plot again with the optimum batch size:
ggplot(sim_results, mapping = aes(x = BatchSize, y = Value)) + 
  geom_point() + 
  stat_smooth(method = lm, formula = y ~ splines::bs(x)) +
  geom_vline(xintercept = Optimum_BatchSize) + 
  geom_text(aes(x=Optimum_BatchSize, label=&amp;quot;\nOptimum Batch Size&amp;quot;, y=9700), angle=90, text=element_text(size=11)) + 
  labs(y = &amp;quot;Output&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Ignoring unknown parameters: text&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-01-making-sense-of-simulation-models-with-metamodels-part-2-splines-with-arena-and-r_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this post, I covered how to apply splines to summarise results from a discrete event simulation model. Splines are a straightforward option to interpolate results from a simulation model, but there are other options out there. Future posts might explore other alternatives such as kriging metamodels and neural nets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Barton2006&#34;&gt;
&lt;p&gt;Barton, Russell R, and Martin Meckesheimer. 2006. “Metamodel-Based Simulation Optimization” 13 (06). &lt;a href=&#34;https://doi.org/10.1016/S0927-0507(06)13018-2&#34;&gt;https://doi.org/10.1016/S0927-0507(06)13018-2&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Making Sense of Simulation Models with Metamodels Part 1 - Low-Order Polynomials with Arena and R</title>
      <link>/post/making-sense-of-models-with-metamodels-low-order-polynomialss-with-arena-and-r/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/making-sense-of-models-with-metamodels-low-order-polynomialss-with-arena-and-r/</guid>
      <description>&lt;p&gt;This is part 1 of a series of posts in which I will explore the utility of using metamodels to make sense of (and possibly optimizing) simulation models.&lt;/p&gt;
&lt;p&gt;If you used simulation modeling on a real project, you might be familiar with this fictional story:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You spent long hours building and refining your simulation model (eg.: a Discrete Event Model). Hopefully, you are confident that it can yield reliable results. Now it’s time to use the model and draw recommendations. At this point, you are probably out of time, the project was delayed by successive rounds of data collection and validation. After running a few scenarios the night before the final presentation, you reach the conclusion that it is going to be hard to explain to your client that the results are highly non-linear and maybe counter-intuitive.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;making-sense-and-possibly-optimizing-models-with-metamodels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Making Sense (and possibly optimizing) Models with Metamodels&lt;/h2&gt;
&lt;p&gt;The idea of building a simple model as a surrogate of a more complicated model might seem analytical overkill. However, long ago, scholars have recognized the utility of using more explicit models to synthesize simulation results, and to find optimal parameters for models with long run time. Refer to &lt;span class=&#34;citation&#34;&gt;(Kleijnen &lt;a href=&#34;#ref-Kleijnen2017&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(Barton and Meckesheimer &lt;a href=&#34;#ref-Barton2006&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt; for comprehensive reviews on Metamodeling for optimization.&lt;/p&gt;
&lt;p&gt;In this post, I will show you how to analyze an Arena Discrete Event Model in R using Low-Order Polynomials.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example-with-arena-and-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Example with Arena and R&lt;/h2&gt;
&lt;p&gt;In this example, the goal is to find an “ideal” batch size, so that our expected output is maximized. Setting the Batch Size “too low”, causes the production system to lose too much time in setups (a setup is required for every batch). Setting the batch size “too high” can cause starvation in other job stations. What “too low” or “too high” means is dependent on various factors, such as cycle times, setup times and other model parameters. Also, improvements in the system may cause the “ideal” batch size to change, but we can’t figure this out without a model.&lt;/p&gt;
&lt;p&gt;Although this example is simple, the underlying idea can be generalized to any case in which a response variable is concave (e.g., Total Costs, Revenue, Throughput) in respect to a decision variable, and your goal is to figure out what this relationship looks like to better manage the system.&lt;/p&gt;
&lt;p&gt;After this introduction, we are going to focus on how to create a metamodel after simulating a few scenarios with Arena.&lt;/p&gt;
&lt;div id=&#34;data-wrangling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Wrangling&lt;/h3&gt;
&lt;p&gt;The first step is obtaining a data.frame where individual observations are simulation replications, and we have one column as the dependent variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and another column as the independent variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, so that we can find a function &lt;span class=&#34;math inline&#34;&gt;\(y = f_{meta}(x)\)&lt;/span&gt; that will provide an aproximation of our model results. This aproximation should be usefull to explain the relationship between &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;First, I simulated all scenarios and saved their results as separate csv files. You can download the files &lt;a href=&#34;/post/2019-03-metamodeling/batchsizefiles.zip&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As you can see opening these files, Arena’s output files need work to become a useful tidy dataframe. By using the package &lt;a href=&#34;arena2r.pedronl.com&#34;&gt;Arena2R&lt;/a&gt;, I can obtain my dataframe easily with the function ‘get_simulation_results’, which will read all csv files in a given path and provide a tidy data.frame with all simulation results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(arena2r)
library(dplyr)
library(ggplot2)
library(readr)

# Obtaining a dataframe compiling all simulation results stored at the &amp;quot;source&amp;quot; folder.
sim_results = arena2r::get_simulation_results(source = &amp;quot;2019-03-metamodeling/&amp;quot;)

head(sim_results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Scenario                   Statistic Replication    Value
## 1 BatchSize200 Colagem.Queue.NumberInQueue           1 9.476321
## 2 BatchSize200 Colagem.Queue.NumberInQueue           2 7.313429
## 3 BatchSize200 Colagem.Queue.NumberInQueue           3 8.647507
## 4 BatchSize200 Colagem.Queue.NumberInQueue           4 7.966887
## 5 BatchSize200 Colagem.Queue.NumberInQueue           5 9.214783
## 6 BatchSize200 Colagem.Queue.NumberInQueue           6 8.143359&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although this dataframe is a good starting point, it does not contain our independent variable (the Batch Size) as a numeric value. I coded my output files so that they will always correspond to BatchSizeXXX, wherein XXX will be a number. After some data wrangling we will be good to continue our metamodeling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Creating a Column For The Dependent Variable, assigning it to the number in the file name:
sim_results$BatchSize = readr::parse_number(as.character(sim_results$Scenario))

# Filter only the Outcome Variable of Interest

sim_results = subset(sim_results, Statistic == &amp;quot;Entity 1.NumberOut&amp;quot;)

# Now Let&amp;#39;s view the relationship between BatchSize and Throughput:

ggplot(sim_results, mapping = aes(x = BatchSize, y = Value, color = Value)) + 
  geom_point() +
  labs(y = &amp;quot;Output&amp;quot;, color = &amp;quot;Output&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-09-making-sense-of-models-with-metamodels-low-order-polynomials-with-arena-and-r_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot shows us important lessons about &lt;em&gt;non-linearity&lt;/em&gt;. Clearly, the output variable has a non-linear relationship with Batch Size. The tricky implication is that if you decided to sample only values of BatchSize &amp;gt; 300, you might reach the conclusion that increasing Batch Size has little impact on Output, and this impact is likely negative. Conversely, if you sample only BatchSize &amp;lt; 300, you would reach the opposite conclusion.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;drawing-curves-revealing-non-linear-patterns&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Drawing Curves, Revealing Non-Linear Patterns&lt;/h2&gt;
&lt;p&gt;If you could draw a curve explaining the relationship between BatchSize and the Output Variable, what would this curve look like? That’s where polynomial metamodels in.&lt;/p&gt;
&lt;p&gt;You can find documentation about polynomial regression in R &lt;a href=&#34;https://www.r-bloggers.com/fitting-polynomial-regression-in-r/&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://medium.com/wwblog/polynomial-regression-in-r-c377f18d6efa&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://www.theanalysisfactor.com/r-tutorial-4/&#34;&gt;here&lt;/a&gt;. Put simply, regression modeling can be seen as drawing lines (or maybe curves) with the purpose of revealing the existence of relationships between variables. In our case, we will first use a polynomial function in the form $y = a + bx + cx^2 $ that will be useful to picture the non-linear relationship between BatchSize and the Output Variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(sim_results, mapping = aes(x = BatchSize, y = Value)) + 
  geom_point() + 
  stat_smooth(method = &amp;quot;lm&amp;quot;, formula = y ~ x + I(x^2), size = 1) + 
  labs(y = &amp;quot;Output&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-09-making-sense-of-models-with-metamodels-low-order-polynomials-with-arena-and-r_files/figure-html/second-order-polynomial-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;optimizing-with-a-quadratic-metamodel&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;“Optimizing” with a Quadratic Metamodel&lt;/h3&gt;
&lt;p&gt;Since our model is quadratic, we can do some calculus to find the point in which the output peaks:&lt;/p&gt;
&lt;p&gt;Since our function is clearly concave down, we can use simple calculus to find the BatchSize Value that maximizes the Output:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_{opt} = \arg\max \ \  ax^2 + bx + c \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can find the optimal point by taking the first derivative:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y&amp;#39; = 2ax + b\]&lt;/span&gt;
Since we know our model is concave down, we know that when the first derivative reaches 0, we will be at its maximum value.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[0 = 2ax_{opt} + b\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_{opt} = -b / 2a\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now that we have a formula, let’s calculate the optimum batch size (based on our metamodel):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quadratic_model = lm(formula = Value ~ BatchSize + I(BatchSize^2), data = sim_results)

## Since our Model is Quadratic, we can derive a formula for the maximum, based on our results

Optimum_BatchSize = - quadratic_model$coefficients[2] / (2 * quadratic_model$coefficients[3])

Optimum_BatchSize&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## BatchSize 
##  342.8003&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does it make sense?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(sim_results, mapping = aes(x = BatchSize, y = Value)) + 
  geom_point() + 
  stat_smooth(method = &amp;quot;lm&amp;quot;, formula = y ~ x + I(x^2), size = 1) +
  geom_vline(xintercept = Optimum_BatchSize) + 
  geom_text(aes(x=Optimum_BatchSize, label=&amp;quot;\nOptimum Batch Size&amp;quot;, y=9700), angle=90, text=element_text(size=11)) + 
  labs(y = &amp;quot;Output&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-09-making-sense-of-models-with-metamodels-low-order-polynomials-with-arena-and-r_files/figure-html/second-order-polynomial-with-optimum-value-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;caveats&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Caveats&lt;/h3&gt;
&lt;p&gt;There are a few caveats you should be aware of when using polynomial metamodels, and I’m citing only two of them here:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Use only low-order polynomials.&lt;/strong&gt; First, if you try a higher order polynomial (for instance, one that includes &lt;span class=&#34;math inline&#34;&gt;\(x^5\)&lt;/span&gt;), you will likely end up with an overfitted model. Try that and see that for yourself.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Avoid using them “Globally”&lt;/strong&gt;: You should avoid using low-order polynomials globally simply because they will become inacurate as you expand the sampling space. Look at the figure above. When Batch Size = 350, the quadratic model over-estimates the Output. There’s a workaround this called “Splines” which will be explored on another post, and there are better options (such as Kriging / Gaussian processes, Neural Nets, etc.).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Using low-order polynomials is a relatively straightforward option you can use to explore and visualize non-linear relationships between decision variables and outcome variables. However, simple polynomial models are limited, and more advanced techniques are available (including Splines, Gaussian Processes, and Neural Nets). The good news is that you can easily find documentation about these techniques in R. Once you have the data, putting together a metamodel in R is usually only a few keystrokes away. In future posts, I will continue to explore increasingly complex metamodels, but keep in mind that the goal should not be to add complexity to the analysis “just because we can”, but to add interpretability and meaning to our results.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Barton2006&#34;&gt;
&lt;p&gt;Barton, Russell R, and Martin Meckesheimer. 2006. “Metamodel-Based Simulation Optimization” 13 (06). &lt;a href=&#34;https://doi.org/10.1016/S0927-0507(06)13018-2&#34;&gt;https://doi.org/10.1016/S0927-0507(06)13018-2&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Kleijnen2017&#34;&gt;
&lt;p&gt;Kleijnen, Jack P C. 2017. “Regression and Kriging metamodels with their experimental designs in simulation : A review.” &lt;em&gt;European Journal of Operational Research&lt;/em&gt; 256 (1). Elsevier B.V.: 1–16. &lt;a href=&#34;https://doi.org/10.1016/j.ejor.2016.06.041&#34;&gt;https://doi.org/10.1016/j.ejor.2016.06.041&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Arena2R - An R Package for Arena Simulation Users</title>
      <link>/post/arena2r-package-tutorial/</link>
      <pubDate>Thu, 27 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/arena2r-package-tutorial/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arenasimulation.com&#34;&gt;Arena Simulation&lt;/a&gt; is a well-known Discrete Event Simulation Software. However, if you are a power user you might want to extend your analysis beyond what Arena’s Process Analyzer offers. In this tutorial, I’ll guide you through the main functions of Arena2R package.&lt;/p&gt;
&lt;p&gt;If you’re not an R user, fear not! Arena2R comes with an app you can use to explore your Arena Simulation data. All you’ll have to do is to Install R and R Studio, and run two commands in your R console.&lt;/p&gt;
&lt;div id=&#34;installation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;You can install arena2r from CRAN with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;arena2r&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, load the package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(arena2r)
## Warning: package &amp;#39;arena2r&amp;#39; was built under R version 3.5.2
library(dplyr)
## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 3.5.2
## 
## Attaching package: &amp;#39;dplyr&amp;#39;
## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag
## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exporting-arena-report-database&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exporting Arena Report Database&lt;/h2&gt;
&lt;p&gt;This is a basic example which shows you how to get your Arena results quickly into R. The basic idea is to run different scenarios and save each of them to a separate csv file. (Yes, you could use Process Analyzer (PAN) to run all scenarios, but to my knowledge, there’s no way to get your data out of the PAN easily).&lt;/p&gt;
&lt;p&gt;Follow these steps to get Arena simulation results to R:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run your model with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; replications. Do not change the number of replications between scenarios.&lt;/li&gt;
&lt;li&gt;For each scenario, save a csv with simulation results clicking on “Tools &amp;gt; ReportDatabase &amp;gt; Export Summary Statistics to CSV File”. Use the standard options. If Arena throws an error, then you’ll have to figure out how to get your results into a csv file. Sometimes it’s necessary to save the report database as a *.mdb file before generating the csv file.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-shiny-app&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using the Shiny App&lt;/h2&gt;
&lt;p&gt;If you’re not familiar to R, you can run this command on R Console and use the example app.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
runArenaApp()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After running this command, the app screen will pop up. You can upload your csv files and play around with the Confidence Interval and Scatter Plots.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-package-with-an-r-script&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using the Package with an R Script&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Open a new .R file, and run the following code:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load the library:

library(arena2r)

# Define the path to your folder with Arena csv files. In my case, it&amp;#39;s here:

my_path = &amp;quot;../../../arena2r/inst/Arena14/&amp;quot;

# Then, get a tidy results data.frame out of your files!
results = arena2r::get_simulation_results(my_path)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also play around with the arena_results dataset included in the package. To use it, follow these steps:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
library(arena2r)

# Load the example dataset:
data(&amp;quot;arena_results&amp;quot;)

# Let&amp;#39;s call it results
results = arena_results

knitr::kable(head(results))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Scenario&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Replication&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NumberIn&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;233&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NumberIn&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;247&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NumberIn&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;239&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NumberIn&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;261&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NumberIn&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;264&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NumberIn&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;266&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;After these steps, now you have a tidy data.frame with your results. Let’s get into possible visualizations. Usually, you’ll be interested in the mean confidence interval for some response variable, across scenarios.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# Plot a Statistic confidence interval across scenarios for a response variable.

arena2r::plot_confint(sim_results = results, response_variable = &amp;quot;Entity 1.NumberOut&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-27-arena2r-package-tutorial_files/figure-html/arena2r-confidence-interval-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now let’s explore the relationship between two variables, across scenarios and replications:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# Now let&amp;#39;s plot analyse the relationship between two variables:

arena2r::plot_scatter(sim_results = results, x_variable = &amp;quot;Entity 1.NumberIn&amp;quot;, y_variable = &amp;quot;Entity 1.NumberOut&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-27-arena2r-package-tutorial_files/figure-html/arena2r-scatter-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now let’s go a bit deeper and leverage ggplot2 to create a plot faceted by Scenario:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# If you use ggplot and you want to get more customized plots, I suggest you to spread your data.frame:

wide_results = results %&amp;gt;%
    tidyr::spread(Statistic, Value)

# Recreating my plot with ggplot, now loking at Resource Utilization:

p = ggplot(data = wide_results, mapping = aes(x = `Resource 1.Utilization`, y = `Entity 1.NumberOut`, color = Scenario)) + geom_point() + facet_wrap(~Scenario)

p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-27-arena2r-package-tutorial_files/figure-html/arena2r-custom-ggplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, let’s summarise every statistic across all scenarios.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
statistics_summary = arena2r::get_statistics_summary(sim_results = results, confidence = 0.95)

knitr::kable(head(statistics_summary[,1:6]))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Scenario&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SD&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Min&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Max&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NumberIn&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;241.03333&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15.773140&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;209.000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;276.0000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NumberOut&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;225.13333&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.735870&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;205.000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;240.0000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.NVATime&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.OtherTime&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.TotalTime&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.15272&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.850762&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.161059&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25.2438&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SCENARIO 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Entity 1.TranTime&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I hope you enjoyed the package. Feel free to suggest new features and to contribute to its development!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>