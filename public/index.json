[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m Pedro\nI am an Associate Engineer RAND Corporation. My research uses modeling to inform health policy. Here you will find links to my recent publications, tools and software I helped to build and more.\nFollow me on twitter and connect on LinkedIn to get updates from my work. I\u0026rsquo;m always interested in meeting people with similar interests, so don\u0026rsquo;t hesitate get in touch if you would like to chat!\n","date":1648462500,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1648462500,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.pedrodelima.com/author/pedro-nascimento-de-lima/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/pedro-nascimento-de-lima/","section":"authors","summary":"I\u0026rsquo;m Pedro\nI am an Associate Engineer RAND Corporation. My research uses modeling to inform health policy. Here you will find links to my recent publications, tools and software I helped to build and more.","tags":null,"title":"Pedro Nascimento de Lima","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\nOnline courses Project or software documentation Tutorials The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://www.pedrodelima.com/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://www.pedrodelima.com/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://www.pedrodelima.com/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":["Pedro Nascimento de Lima"],"categories":null,"content":"","date":1648462500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648462500,"objectID":"47ece450f84ceb49e66b6661120b6f3d","permalink":"https://www.pedrodelima.com/talk/2021-appam/","publishdate":"2021-08-05T00:00:00Z","relpermalink":"/talk/2021-appam/","section":"talk","summary":"The COVID-19 pandemic has disrupted many aspects of life, and several policies have been implemented to lessen the impacts of the COVID-19 pandemic. This session will present papers that evaluate the impacts of these policies. The first paper uses epidemiological and economic models employing the Robust Decision Making (RDM) approach to evaluate the types of policies that are likely to have the largest reductions in COVID-19 transmission. The second paper evaluates the impact of COVID mitigation policies on race/ethnicity disparities. The third paper examines whether the Supplemental Nutrition and Assistance Program (SNAP) and Medicaid interacted to protect against the onset of food, economic, and psychological hardships attributable to the COVID-19 pandemic. The final paper estimate the effect of school reopening policies on COVID-19 transmission. The papers presented in this session further the understanding of the impacts of policies designed to reduce the impacts of the COVID-19 pandemic.","tags":[],"title":"Policy Responses to the COVID-19 Pandemic","type":"talk"},{"authors":null,"categories":null,"content":"","date":1643241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643241600,"objectID":"4b93f0bb10cdefb5c1c030ede9f6df70","permalink":"https://www.pedrodelima.com/project/crcrdm/","publishdate":"2022-01-27T00:00:00Z","relpermalink":"/project/crcrdm/","section":"project","summary":"High-Performance Computing and Robust Decision Making tools for Cancer Screening Models.","tags":["R Package"],"title":"crcrdm","type":"project"},{"authors":["Carolyn M. Rutter","Pedro Nascimento de Lima","Jeffrey K. Lee","Jonathan Ozik"],"categories":null,"content":"","date":1643155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643155200,"objectID":"8a8bb27fc61129fe73f1fbcdd2c7c6fb","permalink":"https://www.pedrodelima.com/publication/2022-too-good-to-be-true-cancer/","publishdate":"2021-10-26T00:00:00Z","relpermalink":"/publication/2022-too-good-to-be-true-cancer/","section":"publication","summary":"This paper examines the validity of the CRC-SPIN model and colonoscopy sensitivity assumptions.","tags":["Simulation Modeling"],"title":"Too Good to Be True? Evaluation of Colonoscopy Sensitivity Assumptions Used in Policy Models","type":"publication"},{"authors":["Katriona Shea","Matthew Biggerstaff","Mike Runge","Pedro Nascimento de Lima","Rebecca Borchering","Robert Lempert"],"categories":null,"content":"","date":1638176400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638176400,"objectID":"6577b80c1191290adc0ad4b14f4e6a9f","permalink":"https://www.pedrodelima.com/talk/2021-dmdu-webinar/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/talk/2021-dmdu-webinar/","section":"talk","summary":"The COVID-19 pandemic has required swift action from public health officials and the infectious disease modeling community. However, deep uncertainties pose challenges both for choosing the best policies and for communicating the rationale behind those choices to the public.  While models provide essential decision support, they should avoid attempts at precise predictions and help account for and hedge against uncertainty. This webinar, hosted by the [Society for Decision Making Under Deep Uncertainty](https://www.deepuncertainty.org/), is an interdisciplinary conversation among researchers who have sought to inform COVID-19 policy with a focus on confronting uncertainty. Michael Runge, Katriona Shea, and Rebecca Borchering will present their work on using [multi-model decision support](https://www.science.org/doi/abs/10.1126/science.abb9934) approaches to simulate [multi-model COVID-19 scenarios](https://www.cdc.gov/mmwr/volumes/70/wr/mm7019e3.htm?s_cid=mm7019e3_w) through the [COVID-19 Scenario Modeling Hub](https://covid19scenariomodelinghub.org/). Pedro Nascimento de Lima and Robert Lempert will discuss their work on using [Robust Decision Making](https://www.rand.org/topics/robust-decision-making.html) to [stress-test reopening strategies](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0259166) and to [evaluate how society might respond to a future pandemic](https://doi.org/10.1101/2021.11.14.21266322). Matthew Biggerstaff, an Epidemiologist at CDC, will present CDC\u0026#39;s use of [modeling to improve the pandemic response](https://pubmed.ncbi.nlm.nih.gov/34343282/). Finally, the panel will discuss lessons learned and remaining challenges for sound decision making under uncertainty during this and a next pandemic.","tags":[],"title":"Informing COVID-19 Policy Under Uncertainty","type":"talk"},{"authors":["Raffaele Vardavas","Pedro Nascimento de Lima"],"categories":null,"content":"","date":1637316000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637316000,"objectID":"d0bcb9e0e7d444038c35679cd597c786","permalink":"https://www.pedrodelima.com/talk/2021-midas-webinar/","publishdate":"2021-11-10T00:00:00Z","relpermalink":"/talk/2021-midas-webinar/","section":"talk","summary":"Changes in behaviors are often acknowledged as a major source of uncertainty in infectious disease modeling work, and there is an[ increasing interest and need for simulation models of ](https://policyandcomplexsystems.files.wordpress.com/2021/09/modeling-infectious-behaviors.pdf)infectious diseases that couple transmission dynamics with adaptive behaviors. Such models could reflect how policies affect behaviors (in both desirable and undesirable ways) concerning subsequent disease epidemiology. This talk presents our path towards tackling behavior (and its inherent uncertainty) in our modeling work. We begin by introducing our COVID-19 [model](https://www.rand.org/pubs/working_papers/WRA1080-1.html) and [analysis](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0259166) and explain how we accounted for some behavioral mechanisms in [our stress-test of COVID-19 reopening policies](https://www.rand.org/pubs/perspectives/PEA1080-1.html). We then describe our prior efforts in developing and informing an ABM to model seasonal influenza vaccination dynamics whereby individuals are influenced by alters in their social network and use inductive reasoning to update their propensity to vaccinate. We will also present publicly available longitudinal surveys that we have designed and fielded through [RAND\u0026#39;s American Life Panel](https://www.rand.org/research/data/alp.html) for the purpose of informing our models. Finally, we discuss our project that will extend this framework for COVID-19 and collect behavioral data on a representative US sample for the next four years. If you are an infectious disease modeler and are interested in the idea of modeling infectious behaviors, please join this talk, and give us feedback! Your feedback may inform how we design future surveys and could result in exciting collaborations.","tags":[],"title":"Towards Modeling Infectious Behaviors","type":"talk"},{"authors":["Sarah A Nowak","Pedro Nascimento de Lima","Raffaele Vardavas"],"categories":null,"content":"","date":1636934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636934400,"objectID":"7625ca81736c85e5761c23309300db57","permalink":"https://www.pedrodelima.com/publication/2021-mitigation-vs-suppression/","publishdate":"2021-11-15T00:00:00Z","relpermalink":"/publication/2021-mitigation-vs-suppression/","section":"publication","summary":"This paper uses a mathematical optimization procedure to find optimal social distancing strategies under a wide range of scenarios.","tags":["COVID-19","Simulation Modeling"],"title":"Should We Mitigate or Suppress the Next Pandemic? Time-Horizons and Costs Shape Optimal Social Distancing Strategies","type":"publication"},{"authors":["Pedro Nascimento de Lima"],"categories":null,"content":"","date":1636619400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636619400,"objectID":"04f75dc776fffdd834a4915b295d8938","permalink":"https://www.pedrodelima.com/talk/2021-cisnet-junior-investigators/","publishdate":"2021-11-10T00:00:00Z","relpermalink":"/talk/2021-cisnet-junior-investigators/","section":"talk","summary":"In this presentation, I introduced RDM to CISNET Junior Investigators and presented our RDM work.","tags":[],"title":"A Primer on Robust Decision Making for Health Policy Modelers","type":"talk"},{"authors":["Pedro Nascimento de Lima","Robert Lempert","Raffaele Vardavas","Lawrence Baker","Jeanne Ringel","Carolyn M. Rutter","Jonathan Ozik","Nicholson Collier"],"categories":null,"content":"","date":1635206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635206400,"objectID":"5525e7554e1a222ef425eb9c5da19c6e","permalink":"https://www.pedrodelima.com/publication/2021-reopening-california-robust-decision-making/","publishdate":"2021-10-26T00:00:00Z","relpermalink":"/publication/2021-reopening-california-robust-decision-making/","section":"publication","summary":"This paper uses the Robust Decision Making approach to stress-test California's COVID-19 reopening strategy considering a range of uncertainties.","tags":["COVID-19","Simulation Modeling","Robust Decision Making"],"title":"Reopening California: Seeking robust, non-Dominated COVID-19 exit strategies","type":"publication"},{"authors":["Raffaele Vardavas","Pedro Nascimento de Lima","Paul K. Davis","Andrew M. Parker","Lawrence Baker"],"categories":null,"content":"","date":1632096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632096000,"objectID":"8db81a4a8239c0b70627a684868a8b81","permalink":"https://www.pedrodelima.com/publication/2021-modeling-infectious-behaviors/","publishdate":"2021-05-09T00:00:00Z","relpermalink":"/publication/2021-modeling-infectious-behaviors/","section":"publication","summary":"This essay discusses the need to account for behavioral adaptation in COVID-19 models.","tags":["COVID-19","Simulation Modeling"],"title":"Modeling Infectious Behaviors: The Need to Account for Behavioral Adaptation in COVID-19 Models","type":"publication"},{"authors":null,"categories":null,"content":"","date":1621555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621555200,"objectID":"c560fdc10c2c8386ac1d7e3416b300ff","permalink":"https://www.pedrodelima.com/project/allegheny-county-covid-19/","publishdate":"2021-05-21T00:00:00Z","relpermalink":"/project/allegheny-county-covid-19/","section":"project","summary":"This tool tracks COVID-19 disparities in Allegheny County.","tags":["Tool"],"title":"Allegheny County COVID-19 Tool","type":"project"},{"authors":["Pedro Nascimento de Lima","Robert Lempert","Raffaele Vardavas","Lawrence Baker","Jeanne Ringel","Carolyn M. Rutter","Jonathan Ozik","Nicholson Collier"],"categories":null,"content":"","date":1620828000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620828000,"objectID":"7d2f5f568538fd3b4120d7782098ee6e","permalink":"https://www.pedrodelima.com/talk/2021-midas-conference/","publishdate":"2021-05-01T00:00:00Z","relpermalink":"/talk/2021-midas-conference/","section":"talk","summary":"Amid global scarcity of COVID-19 vaccines and the threat of new variant strains, California and other jurisdictions face the question of when and how to implement and relax COVID-19 Nonpharmaceutical Interventions (NPIs). While policymakers have attempted to balance the health and economic impacts of the pandemic, decentralized decision-making, deep uncertainty, and the lack of widespread use of comprehensive decision support methods can lead to the choice of fragile or inefficient strategies. This paper uses simulation models and the Robust Decision Making (RDM) approach to stress-test California's reopening strategy and other alternatives over a wide range of futures. We find that plans which respond aggressively to initial outbreaks are required to robustly control the pandemic. Further, the best plans adapt to changing circumstances, lowering their stringent requirements to reopen over time or as more constituents are vaccinated. While we use California as an example, our results are particularly relevant for jurisdictions where vaccination roll-out has been slower.","tags":[],"title":"Reopening California - Seeking Robust, Non-Dominated COVID-19 Exit Strategies","type":"talk"},{"authors":["Pedro Nascimento de Lima","Raffaele Vardavas","Lawrence Baker","Jeanne Ringel","Robert Lempert","Carolyn M. Rutter","Jonathan Ozik"],"categories":null,"content":"","date":1620604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620604800,"objectID":"5105cb320f235fa3f5a38b039101c529","permalink":"https://www.pedrodelima.com/publication/2021-reopening-under-uncertainty-stress-testing-california-covid-19-exit-strategy/","publishdate":"2021-05-09T00:00:00Z","relpermalink":"/publication/2021-reopening-under-uncertainty-stress-testing-california-covid-19-exit-strategy/","section":"publication","summary":"We used the Robust Decision Making approach to stress-test California's COVID-19 reopening strategy. This Perspective presents lessons learned from these experiments and outlines four characteristics of the best reopening strategies.","tags":["COVID-19","Simulation Modeling","Robust Decision Making"],"title":"Reopening Under Uncertainty: Stress-Testing California's COVID-19 Exit Strategy","type":"publication"},{"authors":["Lawrence Baker","Pedro Nascimento de Lima","Jeanne Ringel","Raffaele Vardavas"],"categories":null,"content":"","date":1620518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620518400,"objectID":"3124a20c8c896194e4b2766ab5476096","permalink":"https://www.pedrodelima.com/publication/2021-will-the-us-declare-freedom-from-covid-19-too-soon/","publishdate":"2021-05-09T00:00:00Z","relpermalink":"/publication/2021-will-the-us-declare-freedom-from-covid-19-too-soon/","section":"publication","summary":"We analyzed what could happen with COVID-19 deaths in the United States if restrictions all go away on July 4. Fully reopening the economy before Biden's vaccination target was met doubled the average number of COVID-19 deaths between Independence Day and the end of the year.","tags":["COVID-19","Simulation Modeling","Robust Decision Making"],"title":"Will the United States Declare Freedom from COVID-19 Too Soon?","type":"publication"},{"authors":null,"categories":null,"content":"","date":1619481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619481600,"objectID":"c1323485f9a2f578629a79f2e91aba8d","permalink":"https://www.pedrodelima.com/project/reopening-california/","publishdate":"2021-04-27T00:00:00Z","relpermalink":"/project/reopening-california/","section":"project","summary":"Code Repository for our paper stress-testing California's Reopening strategy.","tags":["R Package","Code"],"title":"Reopening California Code Repository","type":"project"},{"authors":null,"categories":null,"content":"","date":1619395200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619395200,"objectID":"1b634934be918b946d044dcc001f3e08","permalink":"https://www.pedrodelima.com/project/covid-19-state-tool/","publishdate":"2021-04-26T00:00:00Z","relpermalink":"/project/covid-19-state-tool/","section":"project","summary":"A Decision Support Tool to help US states respond to the COVID-19 Pandemic.","tags":["Tool","Visualization"],"title":"COVID-19 Decision Support Tool","type":"project"},{"authors":["Raffaele Vardavas","Pedro Nascimento de Lima","Lawrence Baker"],"categories":null,"content":"","date":1619049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619049600,"objectID":"08cec0d2e03b89b583f02f0f080584ed","permalink":"https://www.pedrodelima.com/publication/2021-modeling-covid-19-npis/","publishdate":"2021-04-22T00:00:00Z","relpermalink":"/publication/2021-modeling-covid-19-npis/","section":"publication","summary":"This paper documents our COVID-19 transmission model and explores the performance of periodic NPI strategies.","tags":["COVID-19","Simulation Modeling"],"title":"Could Periodic Nonpharmaceutical Intervention Strategies Produce Better COVID-19 Health and Economic Outcomes?","type":"publication"},{"authors":["Pedro Nascimento de Lima","David Groves","Robert Lempert","Raffaelle Vardavas","Lawrence Baker","Carlos Calvo-Hernandez"],"categories":null,"content":"","date":1605013200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605013200,"objectID":"b5a8edfd80e89eb54049d5e544196f98","permalink":"https://www.pedrodelima.com/talk/2020-dmdu-meeting/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2020-dmdu-meeting/","section":"talk","summary":"SARS-CoV-2, the virus that causes COVID-19, was first confirmed in the United States on January 19th, 2020, when a man in a Snohomish county urgent care presented with a cough and fever. In the five months since, COVID-19 has spread to every state in the union, with more than 2 million confirmed cases and over 100,000 related deaths. RAND has developed an epidemiological model that describes how Nonpharmaceutical Interventions (NPIs) can delay the spread of the virus, and a general equilibrium model to estimate the economic effects of these interventions. This talk builds on this work and further relaxes structural and parametrical assumptions in this model, employing RDM to investigate i) To what extent current strategies based on NPIs are robust to existing epidemiological and behavioral uncertainty; ii) Under which conditions COVID-19 interventions could fail, and iii) How these strategies could be improved and made more robust. Preliminary findings suggest that strategies based on interrupting economic activity alone as a means of managing outbreaks are not robust to a wide range of futures—that is, they either impose significant economic costs or do not manage infections sufficiently. Including actions that reduce the spread without hindering economic activity—such as mask wearing and other physical distancing measures, have the potential alternatives to improve the robustness of COVID-19 response and recovery strategies, particularly when combined with an adaptive strategy that evolves in response to testing data.","tags":[],"title":"Seeking Robust COVID-19 Response and Adaptation Strategies","type":"talk"},{"authors":["Raffaelle Vardavas","Pedro Nascimento de Lima"],"categories":null,"content":"","date":1604754000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604754000,"objectID":"c67df1603f848a729afd00f3e6170903","permalink":"https://www.pedrodelima.com/talk/2020-informs/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2020-informs/","section":"talk","summary":"We have developed a COVID-19 disease transmission model to help policymakers compare epidemiological outcomes of different nonpharmaceutical interventions (NPIs).  Our model can help predict the timing of peaks in cases, and when to expect a multiple waves based on when and which NPI levels are implemented. In this talk we describe the key epidemiological traits and uncertainties of COVID-19 transmission dynamics, and how they have been considered and evaluated by our epidemiological model. We compare outcomes of different NPI levels and the timing of when they are implemented, and discuss the reasons for which NPIs work best and are robust to the uncertainties in terms of health outcomes.","tags":[],"title":"Comparing COVID-19 Epidemiological Outcomes For Nonpharmaceutical Interventions In The U.S.","type":"talk"},{"authors":["David Groves","Pedro Nascimento de Lima","Raffaelle Vardavas"],"categories":null,"content":"","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591574400,"objectID":"99c6086b21f9ba675ab0d18951be5834","permalink":"https://www.pedrodelima.com/talk/2020-dmdu-webinar/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/2020-dmdu-webinar/","section":"talk","summary":"In this webinar, we discuss our work on modeling COVID-19 and how COVID-19 deep uncertainties can be addressed with the RDM approach.","tags":[],"title":"COVID-19 and Deep Uncertainty","type":"talk"},{"authors":["Raffaele Vardavas","Aaron Strong","Jennifer Bouey","Jonathan William Welburn","Pedro Nascimento de Lima","Lawrence Baker","Keren Zhu","Michelle Priest","Lynn Hu","Jeanne S. Ringel"],"categories":null,"content":"","date":1588636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588636800,"objectID":"75660e19cff1d22b899c82d451e9b737","permalink":"https://www.pedrodelima.com/publication/2020-health-economic-impacts-covid-19-npis/","publishdate":"2020-05-05T00:00:00Z","relpermalink":"/publication/2020-health-economic-impacts-covid-19-npis/","section":"publication","summary":"This model and tool helps local decision makers understand the impacts of Nonpharmaceutical interventions to tackle COVID-19.","tags":["COVID-19","Simulation Modeling"],"title":"The Health and Economic Impacts of Nonpharmaceutical Interventions to Address COVID-19 - A Decision Support Tool for State and Local Policymakers","type":"publication"},{"authors":null,"categories":null,"content":"","date":1587945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587945600,"objectID":"4ea57bd633bd92949c5d9b4ccf76428f","permalink":"https://www.pedrodelima.com/project/ai-health/","publishdate":"2020-04-27T00:00:00Z","relpermalink":"/project/ai-health/","section":"project","summary":"An Evidence Map of Artificial Intelligence applications in Clinical Care.","tags":["Visualization"],"title":"Artificial Intelligence in Clinical Care Evidence Map","type":"project"},{"authors":null,"categories":null,"content":"","date":1587945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587945600,"objectID":"3608a5b8f7e8941ad8c00e702ea84d4d","permalink":"https://www.pedrodelima.com/project/gerbil/","publishdate":"2020-04-27T00:00:00Z","relpermalink":"/project/gerbil/","section":"project","summary":"Gerbil is an R package for multiple imputation to handle missing data.","tags":["R Package"],"title":"Gerbil","type":"project"},{"authors":[],"categories":["Slides"],"content":" Durante o meu tempo como como professor na UNISINOS no curso de Engenharia de Produção, acumulei uma boa quantidade de slides. Provavelmente não vou utilizar estes slides nos próximos 5 anos, portanto nada mais razoável do que compartilhar!\nSimulação Computacional: (por Eventos Discretos, com o Arena): Estes slides contém o conteúdo das aulas de simulação por eventos discretos, utilizando o Arena. Alguma parte deste material foi influenciada pelas aulas dos Profs. Dieter Goldmeyer e Luis Felipe Camargo.\nSistemas de Informação: Esta disciplina englobou uma série de conteúdos interessantes relacionados à Sistemas de Informação para tomada de decisão. O semestre começava com tópicos de Análise de Decisão, e progredia para modelos de machine learning. Na segunda parte do semestre os alunos recebiam a tarefa de excrever o projeto de um sistema de informação relacionado à sua realidade, e recebiam conteúdos relevantes tanto para a gestão de projetos de sistema de informação (ex.: SCRUM, e análise de requisitos), como para a execução prática (ex.: como projetar um banco de dados estruturado e criar um dashboard no Tableau com este banco).\nFundamentos de Sistemas Produtivos (Administração Industrial, Com “a Ciência da Fábrica”): Estes slides contém conteúdo que ministrei para alunos de graduação e MBA em Administração Industrial, e incluem slides sobre a “Ciência da Fábrica”.\n","date":1567468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567468800,"objectID":"c4866e4ceb08e16cb96ba9f3e3ccb2b1","permalink":"https://www.pedrodelima.com/post/slides-graduacao-simulacao-sistemas-de-informacaosistemas-produtivos/","publishdate":"2019-09-03T00:00:00Z","relpermalink":"/post/slides-graduacao-simulacao-sistemas-de-informacaosistemas-produtivos/","section":"post","summary":"Durante o meu tempo como como professor na UNISINOS no curso de Engenharia de Produção, acumulei uma boa quantidade de slides. Provavelmente não vou utilizar estes slides nos próximos 5 anos, portanto nada mais razoável do que compartilhar!","tags":["Arena","Arena Simualtion","Discrete Event Simulation"],"title":"Slides - Simulação (por Eventos Discretos), Sistemas de Informação e outros","type":"post"},{"authors":null,"categories":["R Blogs","R"],"content":" Teaching to seasoned managers in MBE classes is challenging. While it’s important to bring new thoughts and ideas and not sound repetitive, it is necessary to provide a theoretical basis for experienced people with diverse backgrounds. One of the strategies I found to overcome these obstacles this week was to use a new analysis framework (in my case, Factory Physics concepts) to challenge their views about existing frames they already master. Using a combination of concepts, simulation sodels, many-objective tradeoffs visuals (like the gif below) and tasks was great to challenge their intuition about manufacturing systems.\nThis post shares some of the R code I developed while putting together course materials. This post is also an example of how to use simulation metamodeling and Arena to Run Factory Physics’ Flow Benchmarking.\nFlow Benchmarking Flow Benchmarking is an absolute benchmarking technique useful to determine how close a production flow is to its best possible performance. The technique has been introduced in the award-winning Factory Physics (FP) Book (Hopp and Spearman 2008), and is a key component of the science-based manufacturing management approach described in (Pound, Bell, and Spearman 2014).\nDefining Factory Physics Laws The Flow Benchmarking analysis is grounded on Little’s Law (WIP = TH * CT), and utilizes three general cases as absolute benchmarks for any real manufacturing system: The Best Case, the Worst Case and the Practical Worst Case .Please refer to (Hopp and Spearman 2008) and (Pound, Bell, and Spearman 2014) for the rationale for these laws and equations.\nI’ll define these equations as R functions:\ncalc_w0 = function(rb, t0) {rb * t0} ct_best = function(t0, w, w0, rb) {ifelse(w\u0026lt;=w0,t0,w/rb)} th_best = function(t0, w, w0, rb) {ifelse(w\u0026lt;=w0,w/t0,rb)} ct_worst = function(w,t0){w*t0} th_worst = function(t0){1/t0} ct_marginal = function(t0,w,rb){t0+(w-1)/rb} th_marginal = function(w0,w,rb){rb*w/(w0+w-1)} In summary, these equations provide a starting point to discuss how well a manufacturing system is doing in terms of converting inventory to Throughput. The initial analysis requires two inputs. The first input is the Bottleneck rate (rb), which is the production rate (parts, orders / time) of the bottleneck (defined as the process center with the highest long-term utilization). The second parameter is the Total Raw Processing Time (t0), which is the sum of the long-term average process times of each processing center. Based on these two parameters, it’s possible to draw benchmarking curves for the System’s Throughput and Cycle Time as a function of its Work in Process, assuming a CONWIP control system (SPEARMAN, WOODRUFF, and HOPP 1990).\nDrawing Absolute Benchmarking Curves Once I have the basic laws of manufacturing dynamics as R functions, I’ll create a benchmarck_flow function to execute the analysis. This function accepts the rb and t0 parameters and will calculate the system’s Throughput and Cycle time as a function of the wip under different scenarios for benchmarking purposes.\n## Defining Cycle time and Throughput functions benchmark_flow = function(rb, t0, step = 1, wip_mult = 5) { # First, calculate wip_crit w0 = calc_w0(rb = rb, t0 = t0) # Then, define WIP Range to consider: wip = seq.int(from = 1, to = w0 * wip_mult, by = step) # Then, calculate The Best Case Variables Best_Cycle_Time = ct_best(t0 = t0, w = wip, w0 = w0, rb = rb) Best_Throughput = th_best(t0 = t0, w = wip, w0 = w0, rb = rb) best_data = data.frame(WIP = wip, Throughput = Best_Throughput, CycleTime = Best_Cycle_Time, Scenario = \u0026quot;Best Case\u0026quot;) # Calculate the Marginal Cases: Marginal_Cycle_Time = ct_marginal(t0=t0,w=wip,rb=rb) Marginal_Throughput = th_marginal(w0=w0,w=wip,rb=rb) marginal_data = data.frame(WIP = wip, Throughput = Marginal_Throughput, CycleTime = Marginal_Cycle_Time, Scenario = \u0026quot;Marginal\u0026quot;) # Calculate Worst Case worst_data = data.frame( WIP = wip, Throughput = th_worst(t0 = t0), CycleTime = ct_worst(w = wip, t0 = t0), Scenario = \u0026quot;Worst Case\u0026quot; ) # Output A DataFrame with results: # I\u0026#39;m not including the Worst Case because it\u0026#39;s unrealistic (and messes up my cycle time plot). rbind(best_data, marginal_data, worst_data) } # The First Penny Fab Example: data_benchmark = benchmark_flow(rb = 0.5, t0 = 8) knitr::kable(head(data_benchmark)) WIP Throughput CycleTime Scenario 1 0.125 8 Best Case 2 0.250 8 Best Case 3 0.375 8 Best Case 4 0.500 8 Best Case 5 0.500 10 Best Case 6 0.500 12 Best Case How would the Actual System Behave if… Ok, now I have a table with all the basic benchmarking results. What if I have a better model of the system? We can accomplish this by building a discrete event simulation model of the actual system, and using a metamodel of this model to approximate its results (you can find the data from my penny fab model here). During my course, I used several Arena Simulation models to illustrate that adding variability to the system always degrades performance (as the variability law predicts!). Doing so allowed the students to build confidence into the model and the theory, which was great to see!\nlibrary(arena2r) library(tidyr) library(splines) arena_data = arena2r::get_simulation_results(\u0026quot;2019-data/penny-fab\u0026quot;) # Filtering only Statistics of our Interest: filtered_data = subset(arena_data, Statistic %in% c(\u0026quot;w\u0026quot;, \u0026quot;LeadTime\u0026quot;, \u0026quot;Throughput\u0026quot;)) # Spreading and Data Wrangling final_data = filtered_data %\u0026gt;% tidyr::spread(Statistic, Value) %\u0026gt;% dplyr::select(LeadTime, Throughput, w) colnames(final_data) = c(\u0026quot;CycleTime\u0026quot;, \u0026quot;Throughput\u0026quot;, \u0026quot;WIP\u0026quot;) # Now, build a spline metamodel for CycleTime and Throughput as a function of WIP. th_model = lm(Throughput ~ splines::bs(WIP), data = final_data) ct_model = lm(CycleTime ~ WIP, data = final_data) # Put Together a Final DataFrame like the Benchmarking: model_data = data.frame( WIP = unique(data_benchmark$WIP), Throughput = predict(th_model, subset(data_benchmark, Scenario == \u0026quot;Best Case\u0026quot;)), CycleTime = predict(ct_model, subset(data_benchmark, Scenario == \u0026quot;Best Case\u0026quot;)), Scenario = \u0026quot;DES Model\u0026quot; ) ## Warning in splines::bs(WIP, degree = 3L, knots = numeric(0), Boundary.knots = ## c(1, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases # Adding our Model\u0026#39;s Data to the DataFrame: data_benchmark = rbind( data_benchmark, model_data ) Once we have data from the basic FP laws and from our model, let’s plot it!\nlibrary(tidyr) library(ggplot2) library(viridis) ## Loading required package: viridisLite # Lets define a wrapper function for our plot: plot_benchmarking = function(data) { data %\u0026gt;% gather(-WIP, -Scenario, key = \u0026quot;var\u0026quot;, value = \u0026quot;Value\u0026quot;) %\u0026gt;% ggplot(aes(x = WIP, y = Value, color = Scenario)) + geom_line(size = 1) + facet_wrap(~ var, scales = \u0026quot;free\u0026quot;, nrow = 2, ncol = 1) + labs(title = \u0026quot;Flow Benchmarking Plot\u0026quot;) + scale_color_viridis(discrete = TRUE, option = \u0026quot;D\u0026quot;) + theme_bw() } # Then let\u0026#39;s just benchmark and plot! plot_benchmarking(data = data_benchmark) Cool! My simulation metamodel is still quite equivalent to the marginal case.\nHowever, it has one advantage. I can now build a model that can simulate arbitrarily complex scenarios (e,g.: I can include different product routings, change product mix, include non-stationary demand, simulate setup time reduction, even maybe use a multi-method model, etc.) and my model will actually be a better approximation of the actual system than any simple queueing network model. Also, my model can simulate detailed improvement what-if scenarios, which queueing network models won’t be able to simulate.\nWrapping up with Tradeoffs and Many-Objective Visuals I also used simulation models to illustrate tradeoffs implied by two simple decisions: How much WIP a manufacturing flow should have and what should be the reorder level of a part. Unfortunetly, trying to use R to this task wasn’t productive. I ended up using DiscoveryDV, which is a great tool for many-objective visualization.\nFor instance, plotting WIP, Throughput, Cycle Time and Utilization of the Practical Worse Case yields this:\nAnd visualizing the tradeoffs implied by different reorder levels in a (Q,r) inventory system yields this: At this point, many of the participants were excited to get to grips with models that illuminate tradeoffs they have been facing for years. Hopefully, their intuition was sharpened by these exercises and they will be better equiped to use these frontiers to promote productive and tradeoff-aware discussions.\nReferences Hopp, W.J., and M.L. Spearman. 2008. Factory Physics. Irwin/Mcgraw-Hill Series in Operations and Decision Sciences. McGraw-Hill. https://books.google.com.br/books?id=tEjkAAAACAAJ.\nPound, E.S., J.H. Bell, and M.L. Spearman. 2014. Factory Physics for Managers: How Leaders Improve Performance in a Post-Lean Six Sigma World. McGraw-Hill Education. https://books.google.com.br/books?id=B5sXAwAAQBAJ.\nSPEARMAN, MARK L., DAVID L. WOODRUFF, and WALLACE J. HOPP. 1990. “CONWIP: A Pull Alternative to Kanban.” International Journal of Production Research 28 (5): 879–94. https://doi.org/10.1080/00207549008942761.\n","date":1556409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556409600,"objectID":"1ffa943480cab56e4191b6ab04a8eea2","permalink":"https://www.pedrodelima.com/post/teaching-factory-physics-flow-benchmarking-r-many-objective-visuals/","publishdate":"2019-04-28T00:00:00Z","relpermalink":"/post/teaching-factory-physics-flow-benchmarking-r-many-objective-visuals/","section":"post","summary":"Teaching to seasoned managers in MBE classes is challenging. While it’s important to bring new thoughts and ideas and not sound repetitive, it is necessary to provide a theoretical basis for experienced people with diverse backgrounds.","tags":["R","Simulation","Arena","Arena2r"],"title":"Teaching Factory Physics Flow Benchmarking with R and Many-Objective Visuals","type":"post"},{"authors":null,"categories":["R Blogs","R"],"content":" Simulation Metamodeling - building and using surrogate models that can approximate results from more complicated simulation models - is an interesting approach to analyze results from complicated, computationally expensive simulation models. Metamodels are useful because they can yield good approximations of the original simulation model response variables using less computational resources. For an introduction to Metamodeling, refer to (Barton 2015).\nTo my knowledge, no Discrete-Event Simulation (DES) software provides metamodeling capabilities, and guidance on how to actually execute metamodeling is scarce. In this post, I’ll build a Spline-based simulation metamodel. This tutorial should be useful to advanced users of Arena Simulation who would be willing to give metamodeling a try.\nWhy Splines? In my previous post, I briefly described the motivation for using metamodels to approximate simulation models results. Splines are among the useful techniques for metamodeling because: (i) they are relatively simple (they are piecewise-defined polynomials), and (ii) Unlike low-order polynomials, you can generally use them with a global sampling strategy (Barton and Meckesheimer 2006), meaning you can just sample a wide range of input values of your control variable and your model will still have a decent fit.\nData Wrangling Before developing our metamodel, let’s first load the simulation data and do some data wrangling. For the details on this step, please refer to my previous post.\nlibrary(arena2r) library(dplyr) library(ggplot2) library(readr) sim_results = arena2r::get_simulation_results(source = \u0026quot;2019-03-metamodeling/\u0026quot;) sim_results$BatchSize = readr::parse_number(as.character(sim_results$Scenario)) sim_results = subset(sim_results, Statistic == \u0026quot;Entity 1.NumberOut\u0026quot;) head(sim_results) ## Scenario Statistic Replication Value BatchSize ## 51 BatchSize200 Entity 1.NumberOut 1 9200 200 ## 52 BatchSize200 Entity 1.NumberOut 2 9368 200 ## 53 BatchSize200 Entity 1.NumberOut 3 9322 200 ## 54 BatchSize200 Entity 1.NumberOut 4 9039 200 ## 55 BatchSize200 Entity 1.NumberOut 5 9255 200 ## 56 BatchSize200 Entity 1.NumberOut 6 9400 200 Trying Splines You can build a spline model with the R’s standard linear model lm function. Instead of using the standard Y ~ X formula, we just have to use the bs() function from the splines package. Thus, our formula for our spline metamodel will be Y ~ bs(X).\n## Now using Splines: library(splines) # Building a Spline Model: spline_model \u0026lt;-lm(Value ~ bs(BatchSize),data = sim_results) summary(spline_model) ## ## Call: ## lm(formula = Value ~ bs(BatchSize), data = sim_results) ## ## Residuals: ## Min 1Q Median 3Q Max ## -221.646 -30.896 1.011 46.969 268.354 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 9256.49 30.15 306.99 \u0026lt; 2e-16 *** ## bs(BatchSize)1 1312.17 102.27 12.83 \u0026lt; 2e-16 *** ## bs(BatchSize)2 1042.28 88.29 11.80 1.61e-15 *** ## bs(BatchSize)3 961.00 42.95 22.38 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 96.04 on 46 degrees of freedom ## Multiple R-squared: 0.9465, Adjusted R-squared: 0.943 ## F-statistic: 271.1 on 3 and 46 DF, p-value: \u0026lt; 2.2e-16 Once you have your spline_model, you can use the predict function to estimate the expected value of the response variable. Estimating what will be the Expected value of the Output variable with a Batch Size of 200 units is easy as:\npredict(spline_model, newdata = data.frame(BatchSize = 200)) ## 1 ## 9256.489 “Optimizing” with Splines Now that we have a spline model that can approximate our model results, we will use this model to find an “optimal” Batch Size which maximizes our Output Variable.\n## Defining limits: batchlims \u0026lt;- range(sim_results$BatchSize) # Generating Test Data batch.grid\u0026lt;-seq(from=batchlims[1], to = batchlims[2]) # Using the metamodel: spline_data = data.frame(BatchSize = batch.grid, Value = predict(spline_model, newdata = list(BatchSize=batch.grid)) ) # What is the Batch Size which \u0026quot;optimizes\u0026quot; the Output? Optimum_BatchSize \u0026lt;- spline_data$BatchSize[which.max(spline_data$Value)] Output_Value \u0026lt;- spline_data$Value[which.max(spline_data$Value)] Optimum_BatchSize ## [1] 331 The suggested batch size is 331. Is this a reasonable guess, based on our simulation runs? Let’s figure this out by plotting the simulation data, the spline function and the optimum value found.\n# Let\u0026#39;s plot again with the optimum batch size: ggplot(sim_results, mapping = aes(x = BatchSize, y = Value)) + geom_point() + stat_smooth(method = lm, formula = y ~ splines::bs(x)) + geom_vline(xintercept = Optimum_BatchSize) + geom_text(aes(x=Optimum_BatchSize, label=\u0026quot;\\nOptimum Batch Size\u0026quot;, y=9700), angle=90, text=element_text(size=11) ) + labs(y = \u0026quot;Output\u0026quot;) Yes, definitely this is a good estimate! This plot encourages one to avoid going below 300 units, and suggests that going 350 and above is not a good idea either. The interesting pattern that the spline curve suggests is that increasing Batchsize not always increases Output, and that the output loss is not symetric.\nAcknowledging these non-linear relationships is one of the outcomes I value at the end of a simulation project, and I hope that metamodeling will be an useful tool to you as well. Splines are a straightforward option to interpolate results from a simulation model, but there are other options out there. Future posts might explore other alternatives such as kriging metamodels, neural nets, and other techniques.\nReferences Barton, Russell R, and Martin Meckesheimer. 2006. “Metamodel-Based Simulation Optimization” 13 (06). https://doi.org/10.1016/S0927-0507(06)13018-2.\nBarton, Russel R. 2015. “Tutorial: Simulation Metamodeling.” In Proceedings of the 2015 Winter Simulation Conference, 1765–79.\n","date":1555200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555200000,"objectID":"211b4a360e90b18335c8afdfceec580e","permalink":"https://www.pedrodelima.com/post/des-metamodeling-splines-r-arena/","publishdate":"2019-04-14T00:00:00Z","relpermalink":"/post/des-metamodeling-splines-r-arena/","section":"post","summary":"Simulation Metamodeling - building and using surrogate models that can approximate results from more complicated simulation models - is an interesting approach to analyze results from complicated, computationally expensive simulation models.","tags":["Arena Simualtion","Arena2R","Discrete Event Simulation","Metamodeling","R"],"title":"Discrete Event Simulation (DES) Metamodeling - Splines with R and Arena","type":"post"},{"authors":null,"categories":["R","R Blogs"],"content":" This is part 1 of a series of posts in which I will explore the utility of using metamodels to make sense of (and possibly optimizing) simulation models.\nIf you used simulation modeling on a real project, you might be familiar with this fictional story:\nYou spent long hours building and refining your simulation model (eg.: a Discrete Event Model). Hopefully, you are confident that it can yield reliable results. Now it’s time to use the model and draw recommendations. At this point, you are probably out of time, the project was delayed by successive rounds of data collection and validation. After running a few scenarios the night before the final presentation, you reach the conclusion that it is going to be hard to explain to your client that the results are highly non-linear and maybe counter-intuitive.\nMaking Sense (and possibly optimizing) Models with Metamodels The idea of building a simple model as a surrogate of a more complicated model might seem analytical overkill. However, long ago, scholars have recognized the utility of using more explicit models to synthesize simulation results, and to find optimal parameters for models with long run time. Refer to (Kleijnen 2017) and (Barton and Meckesheimer 2006) for comprehensive reviews on Metamodeling for optimization.\nIn this post, I will show you how to analyze an Arena Discrete Event Model in R using Low-Order Polynomials.\nAn Example with Arena and R In this example, the goal is to find an “ideal” batch size, so that our expected output is maximized. Setting the Batch Size “too low”, causes the production system to lose too much time in setups (a setup is required for every batch). Setting the batch size “too high” can cause starvation in other job stations. What “too low” or “too high” means is dependent on various factors, such as cycle times, setup times and other model parameters. Also, improvements in the system may cause the “ideal” batch size to change, but we can’t figure this out without a model.\nAlthough this example is simple, the underlying idea can be generalized to any case in which a response variable is concave (e.g., Total Costs, Revenue, Throughput) in respect to a decision variable, and your goal is to figure out what this relationship looks like to better manage the system.\nAfter this introduction, we are going to focus on how to create a metamodel after simulating a few scenarios with Arena.\nData Wrangling The first step is obtaining a data.frame where individual observations are simulation replications, and we have one column as the dependent variable \\(y\\) and another column as the independent variable \\(x\\), so that we can find a function \\(y = f_{meta}(x)\\) that will provide an aproximation of our model results. This aproximation should be usefull to explain the relationship between \\(x\\) and \\(y\\).\nFirst, I simulated all scenarios and saved their results as separate csv files. You can download the files here.\nAs you can see opening these files, Arena’s output files need work to become a useful tidy dataframe. By using the package Arena2R, I can obtain my dataframe easily with the function ‘get_simulation_results’, which will read all csv files in a given path and provide a tidy data.frame with all simulation results.\nlibrary(arena2r) library(dplyr) library(ggplot2) library(readr) # Obtaining a dataframe compiling all simulation results stored at the \u0026quot;source\u0026quot; folder. sim_results = arena2r::get_simulation_results(source = \u0026quot;2019-03-metamodeling/\u0026quot;) head(sim_results) ## Scenario Statistic Replication Value ## 1 BatchSize200 Colagem.Queue.NumberInQueue 1 9.476321 ## 2 BatchSize200 Colagem.Queue.NumberInQueue 2 7.313429 ## 3 BatchSize200 Colagem.Queue.NumberInQueue 3 8.647507 ## 4 BatchSize200 Colagem.Queue.NumberInQueue 4 7.966887 ## 5 BatchSize200 Colagem.Queue.NumberInQueue 5 9.214783 ## 6 BatchSize200 Colagem.Queue.NumberInQueue 6 8.143359 Although this dataframe is a good starting point, it does not contain our independent variable (the Batch Size) as a numeric value. I coded my output files so that they will always correspond to BatchSizeXXX, wherein XXX will be a number. After some data wrangling we will be good to continue our metamodeling.\n# Creating a Column For The Dependent Variable, assigning it to the number in the file name: sim_results$BatchSize = readr::parse_number(as.character(sim_results$Scenario)) # Filter only the Outcome Variable of Interest sim_results = subset(sim_results, Statistic == \u0026quot;Entity 1.NumberOut\u0026quot;) # Now Let\u0026#39;s view the relationship between BatchSize and Throughput: ggplot(sim_results, mapping = aes(x = BatchSize, y = Value, color = Value)) + geom_point() + labs(y = \u0026quot;Output\u0026quot;, color = \u0026quot;Output\u0026quot;) This plot shows us important lessons about non-linearity. Clearly, the output variable has a non-linear relationship with Batch Size. The tricky implication is that if you decided to sample only values of BatchSize \u0026gt; 300, you might reach the conclusion that increasing Batch Size has little impact on Output, and this impact is likely negative. Conversely, if you sample only BatchSize \u0026lt; 300, you would reach the opposite conclusion.\nDrawing Curves, Revealing Non-Linear Patterns If you could draw a curve explaining the relationship between BatchSize and the Output Variable, what would this curve look like? That’s where polynomial metamodels in.\nYou can find documentation about polynomial regression in R here, here and here. Put simply, regression modeling can be seen as drawing lines (or maybe curves) with the purpose of revealing the existence of relationships between variables. In our case, we will first use a polynomial function in the form $y = a + bx + cx^2 $ that will be useful to picture the non-linear relationship between BatchSize and the Output Variable.\nggplot(sim_results, mapping = aes(x = BatchSize, y = Value)) + geom_point() + stat_smooth(method = \u0026quot;lm\u0026quot;, formula = y ~ x + I(x^2), size = 1) + labs(y = \u0026quot;Output\u0026quot;) “Optimizing” with a Quadratic Metamodel Since our model is quadratic, we can do some calculus to find the point in which the output peaks:\nSince our function is clearly concave down, we can use simple calculus to find the BatchSize Value that maximizes the Output:\n\\[x_{opt} = \\arg\\max \\ \\ ax^2 + bx + c \\]\nWe can find the optimal point by taking the first derivative:\n\\[y\u0026#39; = 2ax + b\\] Since we know our model is concave down, we know that when the first derivative reaches 0, we will be at its maximum value.\n\\[0 = 2ax_{opt} + b\\]\n\\[x_{opt} = -b / 2a\\]\nNow that we have a formula, let’s calculate the optimum batch size (based on our metamodel):\nquadratic_model = lm(formula = Value ~ BatchSize + I(BatchSize^2), data = sim_results) ## Since our Model is Quadratic, we can derive a formula for the maximum, based on our results Optimum_BatchSize = - quadratic_model$coefficients[2] / (2 * quadratic_model$coefficients[3]) Optimum_BatchSize ## BatchSize ## 342.8003 Does it make sense?\nggplot(sim_results, mapping = aes(x = BatchSize, y = Value)) + geom_point() + stat_smooth(method = \u0026quot;lm\u0026quot;, formula = y ~ x + I(x^2), size = 1) + geom_vline(xintercept = Optimum_BatchSize) + geom_text(aes(x=Optimum_BatchSize, label=\u0026quot;\\nOptimum Batch Size\u0026quot;, y=9700), angle=90, text=element_text(size=11)) + labs(y = \u0026quot;Output\u0026quot;) Caveats There are a few caveats you should be aware of when using polynomial metamodels, and I’m citing only two of them here:\n1. Use only low-order polynomials. First, if you try a higher order polynomial (for instance, one that includes \\(x^5\\)), you will likely end up with an overfitted model. Try that and see that for yourself.\n2. Avoid using them “Globally”: You should avoid using low-order polynomials globally simply because they will become inacurate as you expand the sampling space. Look at the figure above. When Batch Size = 350, the quadratic model over-estimates the Output. There’s a workaround this called “Splines” which will be explored on another post, and there are better options (such as Kriging / Gaussian processes, Neural Nets, etc.).\nConclusion Using low-order polynomials is a relatively straightforward option you can use to explore and visualize non-linear relationships between decision variables and outcome variables. However, simple polynomial models are limited, and more advanced techniques are available (including Splines, Gaussian Processes, and Neural Nets). The good news is that you can easily find documentation about these techniques in R. Once you have the data, putting together a metamodel in R is usually only a few keystrokes away. In future posts, I will continue to explore increasingly complex metamodels, but keep in mind that the goal should not be to add complexity to the analysis “just because we can”, but to add interpretability and meaning to our results.\nReferences Barton, Russell R, and Martin Meckesheimer. 2006. “Metamodel-Based Simulation Optimization” 13 (06). https://doi.org/10.1016/S0927-0507(06)13018-2.\nKleijnen, Jack P C. 2017. “Regression and Kriging metamodels with their experimental designs in simulation : A review.” European Journal of Operational Research 256 (1): 1–16. https://doi.org/10.1016/j.ejor.2016.06.041.\n","date":1552089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552089600,"objectID":"128cb6a43f6a541714eb3bfeab51bb3b","permalink":"https://www.pedrodelima.com/post/making-sense-of-models-with-metamodels-low-order-polynomialss-with-arena-and-r/","publishdate":"2019-03-09T00:00:00Z","relpermalink":"/post/making-sense-of-models-with-metamodels-low-order-polynomialss-with-arena-and-r/","section":"post","summary":"This is part 1 of a series of posts in which I will explore the utility of using metamodels to make sense of (and possibly optimizing) simulation models.\nIf you used simulation modeling on a real project, you might be familiar with this fictional story:","tags":["Discrete Event Simulation","R","Metamodeling","Arena Simualtion","Simulation","Arena2R"],"title":"Making Sense of Simulation Models with Metamodels Part 1 - Low-Order Polynomials with Arena and R","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}} Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://www.pedrodelima.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["PN Lima","A Dresch","DP Lacerda"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"13097732d0c112f712bec436bf44e069","permalink":"https://www.pedrodelima.com/publication/2018-socioeconomic-factors/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/2018-socioeconomic-factors/","section":"publication","summary":"There is an increasing body of knowledge on service quality relationship with many contextual factors, including culture, firm size, and public vs. private settings. However, local socioeconomic factors influence towards SMEs Service Quality is still unknown. We conducted statistical analyzes to observe the relationship between contextual socioeconomic factors of an SMEs city and its services quality performance using a SERVPERF survey database of more than 3,000 Brazilian SMEs. While Service Performance did not linearly correlate with the analyzed socioeconomic factors, a closer look at the data shows significant differences in Service Performance among groups of SMEs on highly developed and underdeveloped cities from the other cities. The paper discusses theoretical and managerial implications derived from these findings and proposes new research questions to generate data-backed knowledge to support SMEs service quality improvement.","tags":[],"title":"Do Socioeconomic Contextual Factors Influence SMEs Service Quality? A cross-sector and cross-city SERVPERF Analysis","type":"publication"},{"authors":["A Dresch","DR Veit","PN Lima","DP Lacerda","DC Collato"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"09e18af2035cba5a9ec51a7c6feca897","permalink":"https://www.pedrodelima.com/publication/2018-smes-lean-tools/inducing-brazilian-smes-productivity-lean-manufacturing/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/2018-smes-lean-tools/inducing-brazilian-smes-productivity-lean-manufacturing/","section":"publication","summary":"Purpose – The purpose of this paper is to present a method for assisting micro and small companies of the industrial sector with the adoption of Lean practices. Design/methodology/approach – The paper outlines the method construction steps, which used a design science research approach. Findings – This research led to the structuring of a method for implementing Lean Manufacturing tools in micro and small companies of the industrial sector. The developed method contributed to the knowledge in Lean Manufacturing by systematizing its tools in a heuristic approach that can be applied to an operation using overall equipment effectiveness (OEE) as a guiding indicator. Practical implications – This method can be used to guide the implementation of Lean tools in SMEs industries. Originality/value – The originality of this paper lies in the adoption of an operation-focused approach only (rather than an approach that begins with the mapping of an entire process) and the use of OEE as the basis for prioritization of improvements to be performed and operational control.","tags":[],"title":"Inducing Brazilian manufacturing SMEs productivity with Lean tools","type":"publication"},{"authors":null,"categories":["Conference"],"content":" This post contains materials related to the the presentation given at the 2018 DMDU Annual Meeting.\nFull Master’s Thesis: Donwload Here.\nSlides: Donwload Here.\nPoster: Donwload Here.\n","date":1541376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541376000,"objectID":"0b0de7ec4cfc88fe36977ffe3b6e3b66","permalink":"https://www.pedrodelima.com/post/3d-printing-rdm-analysis-2018-dmdu-meeting/","publishdate":"2018-11-05T00:00:00Z","relpermalink":"/post/3d-printing-rdm-analysis-2018-dmdu-meeting/","section":"post","summary":"This post contains materials related to the the presentation given at the 2018 DMDU Annual Meeting.\nFull Master’s Thesis: Donwload Here.\nSlides: Donwload Here.\nPoster: Donwload Here.","tags":["Simulation"],"title":"Strategic Decision Making in the 3D Printing Industry - 2018 DMDU Annual Meeting Materials","type":"post"},{"authors":null,"categories":["R"],"content":" Este arquivo foi utilizado como arquivo de apoio à aulas ministradas sobre previsão de demanda utilizando o R. Para um tramento mais aprofundado sobre o tema, recorrer ao excelente livro Forecating: Principles and Practice.\nEste post possui uma apresentação relacionada neste link.\nSobre o R O R é um ambiente de computação estatística, com mais de 13 mil pacotes publicados. Cada um destes pacotes tem um fim específico. Neste curso, utilizaremos principalmente as bibliotecas ‘forecast’ e o fpp2. Ambos os pacotes são utlizados no livro Forecating: Principles and Practice. Passos para Realizar Previsões Faça um gráfico dos dados; Selecione uma função para a previsão; Estime os parâmetros da função; Avalie a Qualidade do modelo de previsão; Selecione e implemente o melhor modelo encontrado. Instalando Bibliotecas no R Instale E carregue as bibliotecas que iremos utilizar nesta aula rodando estes comandos: bibliotecas = c(\u0026quot;forecast\u0026quot;, \u0026quot;fpp2\u0026quot;, \u0026quot;readxl\u0026quot;) install.packages(bibliotecas) # Caso encontre algum erro, rode o install packages separadamente para cada biblioteca: install.packages(\u0026quot;forecast\u0026quot;) install.packages(\u0026quot;fpp2\u0026quot;) install.packages(\u0026quot;readxl\u0026quot;) Antes de começar, carregue as bibliotecas:\nlibrary(forecast) library(fpp2) library(readxl) Trabalhando com a Primeira Série Observe as série gold: # A série temporal \u0026quot;gold\u0026quot; foi carregada pela biblioteca forecast. autoplot(gold) Padrões em séries temporais Tendência: Os dados possuem uma tendência geral de aumento ou queda (exemplo: Crescimento da população). Sazonalidade: Os dados se comportam de modo similar, obedecendo padrões com durações fixas (exemplo: venda de ovos de páscoa). Ciclicidade: A série possui um comportamento variando em ciclos, porém sem duração fixa (exemplo: ciclos econômicos.). Observando outras series temporais # Produção de Lâ na Austrália autoplot(woolyrnq) Observando outras series temporais # Produção de Gás na Austrália autoplot(gas) Observando o a Sazonalidade das Séries # A função frequency determina a frequência da série. # Para dados sazonais, irá definir o período dominante da sazonalidade, # e para dados em ciclos, a duração média dos ciclos. frequency(gas) ## [1] 12 Observando Gráficos Sazonais Observando a série temporal:\n# Produção de Gás na Austrália autoplot(a10) + ylab(\u0026quot;Demanda Anti-Diabeticos\u0026quot;) Observando um gráfico sazonal:\nggseasonplot(a10) Observando um gráfico sazonal “polar”:\nggseasonplot(a10, polar = T) Observando a Venda de Cerveja:\nbeer \u0026lt;- window(ausbeer, start = 1992) autoplot(beer) + ylab(\u0026quot;Venda de Cerveja\u0026quot;) Observando a Venda de Cerveja e sua Sazonalidade:\nggseasonplot(beer) Mais uma forma de visualizar a demanda sazonal:\nggsubseriesplot(beer) Criando Objetos ‘ts’ para Séries Temporais a partir de dados reais Para manipular séries temporais no R, é interessante criar objetos do tipo ‘ts’. Podemos criar objetos a partir de arquivos do excel, csv, bases de dados, ou mesmo APIs públicas.\nPodemos utilizar a biblioteca ‘readxl’ para ler arquivos do excel. Obtenha aqui o arquivo de dados VendasCarros.xlsx.\nlibrary(readxl) dados_excel = readxl::read_xlsx(path = \u0026quot;2018-10-10-previsao-serie-temporais/VendasCarros.xlsx\u0026quot;, sheet = \u0026quot;Dados\u0026quot;) head(dados_excel) ## # A tibble: 6 x 4 ## Ano Mês VendasCarros VendasCarrosAux ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2018 1 2493 2486 ## 2 2018 2 2875 3003 ## 3 2018 3 3504 3454 ## 4 2018 4 3786 3665 ## 5 2018 5 4094 4353 ## 6 2018 6 4994 4842 Transformando um Data Frame em uma Série Temporal Para trabalhar com séries temporais, iremos transformar esta tabela: ts_vendas = ts(data = dados_excel$VendasCarros, start = c(2018,1),frequency = 12) ts_vendas ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 2018 2493 2875 3504 3786 4094 4994 4910 5575 5839 6202 6833 7382 ## 2019 2470 3059 3537 4003 4110 4516 5364 5854 5752 6089 7025 7801 ## 2020 2473 2884 3328 3689 4133 4932 5097 5397 6205 6374 6697 8024 Agora que temos uma série temporal, vamos plotar: forecast::autoplot(ts_vendas) Previsões com Modelos de Suavização Exponencial Suavização Exponencial Simples Método da média simples: Leva em consideração todos os períodos; Método “Naive”: Leva em consideração apenas o último período; Método da Suavização Exponencial: Leva em consideração a demanda observada nos períodos anteriores, porém faz com que o impacto dos períodos anteriores caia progressivamente. Notação da Previsão: \\(\\hat{y}_{t+h|t}\\)\nPrevisão para a demanda \\(y_{t+h}\\), considerando dados disponiveis até \\(y_{t}\\)\nEquação da Previsão: \\[\\hat{y}_{t+h|t} = \\alpha y_{t} + \\alpha(1-\\alpha)y_{t-1} + \\alpha(1-\\alpha)^2y_{t-2} + ... \\ para \\ 0 \\leq \\alpha \\leq 1\\]\nModelo de Suavização Exponencial Simples Previsão: \\[\\hat{y}_{t+h|t} = l_t\\] Nível: \\[l_t = \\alpha y_t + (1-\\alpha)l_{t-1}\\] O valor \\(\\alpha\\) é obtido minimizando os erros ao rodar o modelo em um set de teste.\nEstimando o Modelo com o R A função ‘ses’ (Simple Exponetial Smoothing) estima um modelo de suavização exponencial simples: dados_petroleo = window(oil, start= 1996) previsao_petroleo = ses(dados_petroleo, h = 5) # Previsao para os próximos 5 anos summary(previsao_petroleo) ## ## Forecast method: Simple exponential smoothing ## ## Model Information: ## Simple exponential smoothing ## ## Call: ## ses(y = dados_petroleo, h = 5) ## ## Smoothing parameters: ## alpha = 0.8339 ## ## Initial states: ## l = 446.5868 ## ## sigma: 29.8282 ## ## AIC AICc BIC ## 178.1430 179.8573 180.8141 ## ## Error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 6.401975 28.12234 22.2587 1.097574 4.610635 0.9256774 -0.03377748 ## ## Forecasts: ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 2014 542.6806 504.4541 580.9070 484.2183 601.1429 ## 2015 542.6806 492.9073 592.4539 466.5589 618.8023 ## 2016 542.6806 483.5747 601.7864 452.2860 633.0752 ## 2017 542.6806 475.5269 609.8343 439.9778 645.3834 ## 2018 542.6806 468.3452 617.0159 428.9945 656.3667 Visualizando a Previsão: autoplot(previsao_petroleo) Modelo de Suavização Exponencial com Tendência Linear - Holt Previsão: \\[\\hat{y}_{t+h|t} = l_t + h b_t\\] Nível: \\[l_t = \\alpha y_t + (1-\\alpha)(l_{t-1}+b_{t-1})\\] Tendência: \\[b_t = \\beta *(l_t-l_{t-1}) + (1-\\beta)b_{t-1}\\]\nEstimando o Modelo com o R A função ‘holt’ (Simple Exponetial Smoothing) estima o modelo de Holt (que considera a tendência): previsao_petroleo_holt = holt(dados_petroleo, h = 5) # Previsao para os próximos 5 anos autoplot(previsao_petroleo_holt) Também podemos observar os parâmetros estimados para o modelo: summary(previsao_petroleo_holt) ## ## Forecast method: Holt\u0026#39;s method ## ## Model Information: ## Holt\u0026#39;s method ## ## Call: ## holt(y = dados_petroleo, h = 5) ## ## Smoothing parameters: ## alpha = 1e-04 ## beta = 1e-04 ## ## Initial states: ## l = 428.4833 ## b = 5.5771 ## ## sigma: 27.8684 ## ## AIC AICc BIC ## 177.2927 182.2927 181.7446 ## ## Error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set -0.2851768 24.57757 20.53231 -0.3285466 4.337459 0.8538816 ## ACF1 ## Training set 0.3429178 ## ## Forecasts: ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 2014 534.4382 498.7234 570.1529 479.8172 589.0591 ## 2015 540.0148 504.3000 575.7295 485.3938 594.6357 ## 2016 545.5913 509.8766 581.3061 490.9704 600.2123 ## 2017 551.1679 515.4532 586.8827 496.5469 605.7889 ## 2018 556.7445 521.0298 592.4592 502.1235 611.3655 Modelo de Holt com “Damp” Podemos utilizar o modelo Holt com “Damp”, pressupondo que o crescimento não será linear no longo prazo. Previsão: \\[\\hat{y}_{t+h|t} = l_t + (\\phi+\\phi^2+...+\\phi^h) b_t\\]\nNível: \\[l_t = \\alpha y_t + (1-\\alpha)(l_{t-1}+\\phi b_{t-1})\\]\nTendência: \\[b_t = \\beta *(l_t-l_{t-1}) + (1-\\beta) \\phi b_{t-1}\\] O parâmetro \\(\\phi\\) é entre 0 e 1. Se o parâmetro for igual a 1, o crescimento será linear.\nUtilizando o Modelo de Holt com “Damp”:\nprevisao_petroleo_holt = holt(dados_petroleo, h = 15, PI = F) previsao_petroleo_holt_damp = holt(dados_petroleo, h = 15, damped = T, PI = F) # Previsao para os próximos 5 anos autoplot(dados_petroleo) + autolayer(previsao_petroleo_holt, series=\u0026quot;Holt Linear\u0026quot;) + autolayer(previsao_petroleo_holt_damp, series=\u0026quot;Holt com Damp\u0026quot;) Modelo de Suavização Exponencial com Tendência e Sazonalidade - Holt-Winters Método Aditivo: Adequado quando a amplitude dos ciclos de sazonalidade não está correlacionada ao tempo. Método Multiplicativo: Adequado quando a amplitude dos ciclos de sazonalidade está correlacionadao ao tempo. Modelo Holt-Winters Aditivo Previsão: \\[\\hat{y}_{t+h|t} = l_t + hb_t + s_{t-m+h_m^+}\\]\nNível: \\[l_t = \\alpha (y_t-s_{t-m}) + (1-\\alpha)(l_{t-1}+b_{t-1})\\]\nTendência: \\[b_t = \\beta *(l_t-l_{t-1}) + (1-\\beta) b_{t-1}\\]\nSazonalidade: \\[b_t = \\gamma(y_t - l_{t-1 - b_{t-1}}) + (1-\\gamma)s_{t-m} \\] \\(s_{t-m+h_m^+}\\): componente de sazonalidade do último ano de dados disponíveis. \\(m\\): Período de sazonalidade. A média do componente de sazonalidade tende a zero.\nModelo Holt-Winters Multiplicatio Previsão: \\[\\hat{y}_{t+h|t} = (l_t + hb_t) s_{t-m+h_m^+}\\]\nNível: \\[l_t = \\alpha \\frac{y_t}{s_{t-m}} + (1-\\alpha)(l_{t-1}+b_{t-1})\\]\nTendência: \\[b_t = \\beta *(l_t-l_{t-1}) + (1-\\beta) b_{t-1}\\]\nSazonalidade: \\[b_t = \\gamma \\frac{y_t}{l_{t-1} + b_{t-1}} + (1-\\gamma)s_{t-m} \\] \\(s_{t-m+h_m^+}\\): componente de sazonalidade do último ano de dados disponíveis. \\(m\\): Período de sazonalidade. A média do componente de sazonalidade tende a 1.\nExemplo - Holt-Winters Aditivo Relembrando a série de anti-glicêmicos.\n# Produção de Gás na Austrália autoplot(a10) + ylab(\u0026quot;Demanda AntiDiabeticos\u0026quot;) Realizando a Previsão:\n# Produção de Gás na Austrália previsao_anti_diab_aditivo = hw(a10, seasonal = \u0026quot;additive\u0026quot;, PI= F) previsao_anti_diab_multiplicativo = hw(a10, seasonal = \u0026quot;multiplicative\u0026quot;, PI = F) autoplot(a10) + ylab(\u0026quot;Demanda de Remédios\u0026quot;) + autolayer(previsao_anti_diab_aditivo, series=\u0026quot;HW Add.\u0026quot;) + autolayer(previsao_anti_diab_multiplicativo, series=\u0026quot;HW Mult.\u0026quot;) Desafio: Colete dados de demanda de diferentes produts em sua empresa; Verifique quais são as técnicas de previsão aplicadas a estes produtos; Identifique se há ou não tendência e sazonalidade; Aplique os métodos aprendidos; Proponha um modelo de previsão novo; Compare a acurácia do novo modelo em relação ao que existe atualmente na empresa. ARIMA O modelo ARIMA não será coberto nesta aula, porém a biblioteca forecast possui uma função para estimar modelos Arima de modo automático.\nmodelo_arima = forecast::auto.arima(a10) autoplot(forecast(modelo_arima)) Mais Aspectos Técnicos O modelo “Naive” O modelo “naive” simplesmente pressupõe que o futuro repetirá o passado, logo a previsão é correspondente ao último valor observado. previsao = naive(oil) autoplot(oil, series=\u0026quot;Dados\u0026quot;) + xlab(\u0026quot;Ano\u0026quot;) + autolayer(fitted(previsao), series = \u0026quot;Previsao\u0026quot;) + ggtitle(\u0026quot;Produção de Petróleo na Arábia Saudita\u0026quot;) Observando os Erros Os erros também podem ser plotados. Espera-se que os erros sejam “white noise”. Se isso é verdade, é provável que o modelo tenha captado toda a informação disponível nos dados. autoplot(residuals(previsao)) Pressupostos sobre os Erros Os erros deveriam ser não-correlacionados; Os erros devem ter média zero; Propriedades úteis: Variância Constante; São normalmente distribuídos. Verificando Pressupostos sobre os Erros A função ‘checkresiduals’ verifica os pressupostos indicados anteriormente. Espera-se que o resultado do p-valor do Ljung-box seja acima de 0.05, indicando que os erros não são correlacionados. checkresiduals(previsao) ## ## Ljung-Box test ## ## data: Residuals from Naive method ## Q* = 12.59, df = 10, p-value = 0.2475 ## ## Model df: 0. Total lags used: 10 O que fazer se os erros não forem normais? Ainda assim, as previsões podem ser boas e podem ser utilizadas, porém os intervalos de predição podem ser muito justos ou amplos em função deste problema. Training and Test Sets Traning Set: Parte dos dados que você utiliza para construir o modelo. Test Set: Parte dos dados que você utiliza para testar o modelo. O test set não pode ser usado para calcular a previsão. Um modelo que se ajusta bem aos dados de treinamento não necessáriamente terá uma boa previsão; É comum construirmos um modelo altamente complexo que possui poucos erros nos dados de treinamento, porém gera péssimas previsões. Isto é chamado de overfitting, e deve ser evitado. Medidas de Acurácia da Previsão Mean Absolute Error: \\(MAE = avg(|e_t|)\\) Mean Square Error: \\(MAE = avg(|e_t^2|)\\) Mean Absolute Percentage Error: \\(MAPE = 100 * avg(|e_t/y_t|)\\) Mean Absolute Scaled Error: \\(MASE = MAE / Q\\) Exemplo com um Modelo Naive Separamos o modelo em duas partes, e geramos um modelo naive para ilustrar este ponto: treinamento = window(oil, end=2003) teste = window(oil, start= 2004) previsao = naive(treinamento,h = 10) # h = Número de períodos a prever autoplot(previsao) + ylab(\u0026quot;Vendade Petróleo\u0026quot;) + autolayer(teste, series = \u0026quot;Dados de Teste\u0026quot;) O Comando Accuracy Usar o Comando Accuracy para observar o modelo: accuracy(previsao, teste) ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 9.87358 52.56156 39.42504 2.506565 12.570647 1.0000000 0.1801528 ## Test set 21.60250 35.09832 29.97666 3.963914 5.777875 0.7603458 0.4029519 ## Theil\u0026#39;s U ## Training set NA ## Test set 1.184862 Obtendo Mais Séries de Dados reais: O agregador de dados Quandl possui milhares de séries temporais disponíveis diretamente no R, pela biblioteca Quandl. Exemplo: obtendo a série do índice Bovespa library(Quandl) ts_bovespa = Quandl(\u0026quot;BCB/7\u0026quot;, type = \u0026quot;ts\u0026quot;) autoplot(ts_bovespa) ","date":1539129600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539129600,"objectID":"da2320b17afc43924bb4d2237430d660","permalink":"https://www.pedrodelima.com/post/previsao-de-series-temporais-com-o-r/","publishdate":"2018-10-10T00:00:00Z","relpermalink":"/post/previsao-de-series-temporais-com-o-r/","section":"post","summary":"Este arquivo foi utilizado como arquivo de apoio à aulas ministradas sobre previsão de demanda utilizando o R. Para um tramento mais aprofundado sobre o tema, recorrer ao excelente livro Forecating: Principles and Practice.","tags":["Previsão de Demanda","Séries Temporais","Suavização Exponencial"],"title":"Previsão de Séries Temporais com o R","type":"post"},{"authors":null,"categories":["R","R Blogs"],"content":" Arena Simulation is a well-known Discrete Event Simulation Software. However, if you are a power user you might want to extend your analysis beyond what Arena’s Process Analyzer offers. In this tutorial, I’ll guide you through the main functions of Arena2R package.\nIf you’re not an R user, fear not! Arena2R comes with an app you can use to explore your Arena Simulation data. All you’ll have to do is to Install R and R Studio, and run two commands in your R console.\nInstallation You can install arena2r from CRAN with:\ninstall.packages(\u0026quot;arena2r\u0026quot;) Then, load the package:\nlibrary(arena2r) library(dplyr) ## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union library(ggplot2) Exporting Arena Report Database This is a basic example which shows you how to get your Arena results quickly into R. The basic idea is to run different scenarios and save each of them to a separate csv file. (Yes, you could use Process Analyzer (PAN) to run all scenarios, but to my knowledge, there’s no way to get your data out of the PAN easily).\nFollow these steps to get Arena simulation results to R:\nRun your model with \\(n\\) replications. Do not change the number of replications between scenarios. For each scenario, save a csv with simulation results clicking on “Tools \u0026gt; ReportDatabase \u0026gt; Export Summary Statistics to CSV File”. Use the standard options. If Arena throws an error, then you’ll have to figure out how to get your results into a csv file. Sometimes it’s necessary to save the report database as a *.mdb file before generating the csv file. Using the Shiny App If you’re not familiar to R, you can run this command on R Console and use the example app.\nrunArenaApp() After running this command, the app screen will pop up. You can upload your csv files and play around with the Confidence Interval and Scatter Plots.\nUsing the Package with an R Script Open a new .R file, and run the following code: # Load the library: library(arena2r) # Define the path to your folder with Arena csv files. In my case, it\u0026#39;s here: my_path = \u0026quot;../../../arena2r/inst/Arena14/\u0026quot; # Then, get a tidy results data.frame out of your files! results = arena2r::get_simulation_results(my_path) You can also play around with the arena_results dataset included in the package. To use it, follow these steps:\nlibrary(arena2r) # Load the example dataset: data(\u0026quot;arena_results\u0026quot;) # Let\u0026#39;s call it results results = arena_results knitr::kable(head(results)) Scenario Statistic Replication Value SCENARIO 1 Entity 1.NumberIn 1 233 SCENARIO 1 Entity 1.NumberIn 2 247 SCENARIO 1 Entity 1.NumberIn 3 239 SCENARIO 1 Entity 1.NumberIn 4 261 SCENARIO 1 Entity 1.NumberIn 5 264 SCENARIO 1 Entity 1.NumberIn 6 266 After these steps, now you have a tidy data.frame with your results. Let’s get into possible visualizations. Usually, you’ll be interested in the mean confidence interval for some response variable, across scenarios.\n# Plot a Statistic confidence interval across scenarios for a response variable. arena2r::plot_confint(sim_results = results, response_variable = \u0026quot;Entity 1.NumberOut\u0026quot;) ## Warning: `fun.y` is deprecated. Use `fun` instead. ## Warning: Computation failed in `stat_summary()`: ## Hmisc package required for this function Now let’s explore the relationship between two variables, across scenarios and replications:\n# Now let\u0026#39;s plot analyse the relationship between two variables: arena2r::plot_scatter(sim_results = results, x_variable = \u0026quot;Entity 1.NumberIn\u0026quot;, y_variable = \u0026quot;Entity 1.NumberOut\u0026quot;) Now let’s go a bit deeper and leverage ggplot2 to create a plot faceted by Scenario:\n# If you use ggplot and you want to get more customized plots, I suggest you to spread your data.frame: wide_results = results %\u0026gt;% tidyr::spread(Statistic, Value) # Recreating my plot with ggplot, now loking at Resource Utilization: p = ggplot(data = wide_results, mapping = aes(x = `Resource 1.Utilization`, y = `Entity 1.NumberOut`, color = Scenario)) + geom_point() + facet_wrap(~Scenario) p Finally, let’s summarise every statistic across all scenarios.\nstatistics_summary = arena2r::get_statistics_summary(sim_results = results, confidence = 0.95) knitr::kable(head(statistics_summary[,1:6])) Scenario Statistic Mean SD Min Max SCENARIO 1 Entity 1.NumberIn 241.03333 15.773140 209.000000 276.0000 SCENARIO 1 Entity 1.NumberOut 225.13333 7.735870 205.000000 240.0000 SCENARIO 1 Entity 1.NVATime 0.00000 0.000000 0.000000 0.0000 SCENARIO 1 Entity 1.OtherTime 0.00000 0.000000 0.000000 0.0000 SCENARIO 1 Entity 1.TotalTime 11.15272 4.850762 5.161059 25.2438 SCENARIO 1 Entity 1.TranTime 0.00000 0.000000 0.000000 0.0000 I hope you enjoyed the package. Feel free to suggest new features and to contribute to its development!\n","date":1538006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538006400,"objectID":"707456c140b767a62d61d9bd592d9cbd","permalink":"https://www.pedrodelima.com/post/arena2r-package-tutorial/","publishdate":"2018-09-27T00:00:00Z","relpermalink":"/post/arena2r-package-tutorial/","section":"post","summary":"Arena Simulation is a well-known Discrete Event Simulation Software. However, if you are a power user you might want to extend your analysis beyond what Arena’s Process Analyzer offers. In this tutorial, I’ll guide you through the main functions of Arena2R package.","tags":["Arena Simualtion","Discrete Event Simulation","R Package","R"],"title":"Arena2R - An R Package for Arena Simulation Users","type":"post"},{"authors":["DR Veit","DP Lacerda","MIWM Morandi","A Dresch","LH Rodrigues","PN Lima"],"categories":null,"content":"","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"0bfc19073dafc8e1eee2a2db205f7ba4","permalink":"https://www.pedrodelima.com/publication/2019-additive-manufacturing-impacts/","publishdate":"2018-09-01T00:00:00Z","relpermalink":"/publication/2019-additive-manufacturing-impacts/","section":"publication","summary":"Additive Manufacturing has been growing in adoption in the last decades, holding the potential to fundamentally transform traditional production systems. Yet, its prospected impact on Production Systems and Operations Strategy is controversial. This article contributes to this debate by examining and summarizing the prospected impact of Additive Manufacturing on Production Systems, in the light of the Competitive Dimensions through a Systematic Literature Review. In particular, this paper organises this debate by highlighting the specific impacts prospected by the literature for each competitive dimension.","tags":[],"title":"The Impacts of Additive Manufacturing on Production Systems","type":"publication"},{"authors":null,"categories":["R"],"content":" Este post corresponde aos slides do Mini-Curso “Introdução ao R - Aprendendo com o Moneyball”, realizado em 2017 no SIGEPRO. Cada um dos títulos neste post correspondia a um slide.\nO que Veremos neste Mini-Curso? O que é Data Analytics? Exemplo Moneyball. Como continuar aprendendo. Porque usar o R ? Open Source e Gratuito; Mais de 10 k bibliotecas gratuitas; Suporta muitos tipos de Análises; Conhecimento “cumulativo” e transferível a outros contextos. Outras alternativas se você não quiser programar. Excel(?); Alteryx; Microsoft Azure; Tableu para Análises visuais mais simples; Que tipo de pessoa usa o R? R Developer: “Um desenvolvedor R usa suas habilidades de programamação para manipular dados e construir ferramentas para para análise de Dados.” Data Scientist: “Um cientista de dados combina técnicas estatísticas e de machine learning com programação em R para analisar e interpretar dados complexos”. Data Analyst: Um Data Analyst traduz números em português claro. Um analista de dados interpreta dados das empresas e o usa para tomar melhores decisões. Analista Quantitativo: Na área financeira, uma analista quantitativo garante que portfolios de investimento estão balenceados e encontra novas oportunidades de trading, e avalia preços de ativos usando modelos matemáticos. Fonte: http://datacamp.com Exemplo - MoneyBall Este exemplo usa dados relacionados ao filme “Moneyball” para apresentar a técnica de regressão linear com o R. Este exercício e a ideia de usar o exemplo do Moneyball é baseda em uma aula do MIT, da plataforma Edx: https://courses.edx.org/courses/course-v1:MITx+15.071x_3+1T2016/\nMoneyball e o Oakland A’s Moneyball é o livro que conta a história sobre como o Data Analytics mudou a indústria do baseball; Oakland A’s: Um dos times mais pobres do baseball. Foi vendido e teve seu orçamento cortado; Em 2002 o time perdeu três jogadores principais (é desse ponto que o filme começa); Qual é a meta de um time de Baseball? Ir para as Playoffs! Quantos jogos um time precisa ganhar para chegar às playoffs? Paul DePodesta calculou que um time precisa de 95 vitórias para chegar às Playoffs. Como se vence 95 jogos? Fazendo mais “Runs” do que o oponente. Quantos “Runs” a mais? Eles calcularam que precisariam fazer 135 Runs a mais do receberam para ganhar 95 jogos. Como calcular isso? Lendo Dados em CSV com read.csv() Normalmente lemos dados no formato .csv no R para realizar as análises. É possível também ler dados em outros formatos. Obtenha aqui o arquivo baseball.\n# Lendo Dados em CSV baseball \u0026lt;- read.csv(\u0026quot;2018-10-10-moneyball-r/baseball.csv\u0026quot;) Conhecendo os Dados com str() Antes de rodar qualquer análise precisamos conhecer a estrutura dos dados. Os dados contém uma linha para cada time e ano de 1962 a 2012 para todas as temporadas. Dados incluem Runs Scored (RS), Runs Allowed (RA) e Vitórias (W).\n# Podemos fazer isso usando a função str() (que mostra a estrutura) str(baseball) ## \u0026#39;data.frame\u0026#39;: 1232 obs. of 15 variables: ## $ Team : chr \u0026quot;ARI\u0026quot; \u0026quot;ATL\u0026quot; \u0026quot;BAL\u0026quot; \u0026quot;BOS\u0026quot; ... ## $ League : chr \u0026quot;NL\u0026quot; \u0026quot;NL\u0026quot; \u0026quot;AL\u0026quot; \u0026quot;AL\u0026quot; ... ## $ Year : int 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ... ## $ RS : int 734 700 712 734 613 748 669 667 758 726 ... ## $ RA : int 688 600 705 806 759 676 588 845 890 670 ... ## $ W : int 81 94 93 69 61 85 97 68 64 88 ... ## $ OBP : num 0.328 0.32 0.311 0.315 0.302 0.318 0.315 0.324 0.33 0.335 ... ## $ SLG : num 0.418 0.389 0.417 0.415 0.378 0.422 0.411 0.381 0.436 0.422 ... ## $ BA : num 0.259 0.247 0.247 0.26 0.24 0.255 0.251 0.251 0.274 0.268 ... ## $ Playoffs : int 0 1 1 0 0 0 1 0 0 1 ... ## $ RankSeason : int NA 4 5 NA NA NA 2 NA NA 6 ... ## $ RankPlayoffs: int NA 5 4 NA NA NA 4 NA NA 2 ... ## $ G : int 162 162 162 162 162 162 162 162 162 162 ... ## $ OOBP : num 0.317 0.306 0.315 0.331 0.335 0.319 0.305 0.336 0.357 0.314 ... ## $ OSLG : num 0.415 0.378 0.403 0.428 0.424 0.405 0.39 0.43 0.47 0.402 ... Resumindo com o summary() Também podemos ter uma ideia dos dados usando o summary. Ele nos retorna médias, quartis, valores mínimos e máximos.\nsummary(baseball) ## Team League Year RS ## Length:1232 Length:1232 Min. :1962 Min. : 463.0 ## Class :character Class :character 1st Qu.:1977 1st Qu.: 652.0 ## Mode :character Mode :character Median :1989 Median : 711.0 ## Mean :1989 Mean : 715.1 ## 3rd Qu.:2002 3rd Qu.: 775.0 ## Max. :2012 Max. :1009.0 ## ## RA W OBP SLG ## Min. : 472.0 Min. : 40.0 Min. :0.2770 Min. :0.3010 ## 1st Qu.: 649.8 1st Qu.: 73.0 1st Qu.:0.3170 1st Qu.:0.3750 ## Median : 709.0 Median : 81.0 Median :0.3260 Median :0.3960 ## Mean : 715.1 Mean : 80.9 Mean :0.3263 Mean :0.3973 ## 3rd Qu.: 774.2 3rd Qu.: 89.0 3rd Qu.:0.3370 3rd Qu.:0.4210 ## Max. :1103.0 Max. :116.0 Max. :0.3730 Max. :0.4910 ## ## BA Playoffs RankSeason RankPlayoffs ## Min. :0.2140 Min. :0.0000 Min. :1.000 Min. :1.000 ## 1st Qu.:0.2510 1st Qu.:0.0000 1st Qu.:2.000 1st Qu.:2.000 ## Median :0.2600 Median :0.0000 Median :3.000 Median :3.000 ## Mean :0.2593 Mean :0.1981 Mean :3.123 Mean :2.717 ## 3rd Qu.:0.2680 3rd Qu.:0.0000 3rd Qu.:4.000 3rd Qu.:4.000 ## Max. :0.2940 Max. :1.0000 Max. :8.000 Max. :5.000 ## NA\u0026#39;s :988 NA\u0026#39;s :988 ## G OOBP OSLG ## Min. :158.0 Min. :0.2940 Min. :0.3460 ## 1st Qu.:162.0 1st Qu.:0.3210 1st Qu.:0.4010 ## Median :162.0 Median :0.3310 Median :0.4190 ## Mean :161.9 Mean :0.3323 Mean :0.4197 ## 3rd Qu.:162.0 3rd Qu.:0.3430 3rd Qu.:0.4380 ## Max. :165.0 Max. :0.3840 Max. :0.4990 ## NA\u0026#39;s :812 NA\u0026#39;s :812 Acessando variáveis específicas de um DataFrame Podemos acessar variáveis específicas de um Data Frame usando algumas notações possíveis.\nhead(baseball$Year) ## [1] 2012 2012 2012 2012 2012 2012 Selecionando Linhas Específicas do DF Vamos selecionar apenas dados até o ano de 2002 (que foram os dados que eles possuíam em 2002)\n# Considerando apenas anos exibidos pelo moneyball moneyball = subset(baseball, Year \u0026lt; 2002) str(moneyball) ## \u0026#39;data.frame\u0026#39;: 902 obs. of 15 variables: ## $ Team : chr \u0026quot;ANA\u0026quot; \u0026quot;ARI\u0026quot; \u0026quot;ATL\u0026quot; \u0026quot;BAL\u0026quot; ... ## $ League : chr \u0026quot;AL\u0026quot; \u0026quot;NL\u0026quot; \u0026quot;NL\u0026quot; \u0026quot;AL\u0026quot; ... ## $ Year : int 2001 2001 2001 2001 2001 2001 2001 2001 2001 2001 ... ## $ RS : int 691 818 729 687 772 777 798 735 897 923 ... ## $ RA : int 730 677 643 829 745 701 795 850 821 906 ... ## $ W : int 75 92 88 63 82 88 83 66 91 73 ... ## $ OBP : num 0.327 0.341 0.324 0.319 0.334 0.336 0.334 0.324 0.35 0.354 ... ## $ SLG : num 0.405 0.442 0.412 0.38 0.439 0.43 0.451 0.419 0.458 0.483 ... ## $ BA : num 0.261 0.267 0.26 0.248 0.266 0.261 0.268 0.262 0.278 0.292 ... ## $ Playoffs : int 0 1 1 0 0 0 0 0 1 0 ... ## $ RankSeason : int NA 5 7 NA NA NA NA NA 6 NA ... ## $ RankPlayoffs: int NA 1 3 NA NA NA NA NA 4 NA ... ## $ G : int 162 162 162 162 161 162 162 162 162 162 ... ## $ OOBP : num 0.331 0.311 0.314 0.337 0.329 0.321 0.334 0.341 0.341 0.35 ... ## $ OSLG : num 0.412 0.404 0.384 0.439 0.393 0.398 0.427 0.455 0.417 0.48 ... Relembrando nosso objetivo Objetivo: Saber como chegar às playoffs. Em outras Palavras: Saber quantos Runs um time deve fazer a mais do que “leva” para ter mais do que 95 vitórias. Como: Usando uma Regressão Linear para predizer Vitórias em função do Run Differences. Calculando a RUN Difference Criando uma nova variável para calcular a “Run Difference”\n# Calculando a Run Difference moneyball$RD = moneyball$RS - moneyball$RA str(moneyball) ## \u0026#39;data.frame\u0026#39;: 902 obs. of 16 variables: ## $ Team : chr \u0026quot;ANA\u0026quot; \u0026quot;ARI\u0026quot; \u0026quot;ATL\u0026quot; \u0026quot;BAL\u0026quot; ... ## $ League : chr \u0026quot;AL\u0026quot; \u0026quot;NL\u0026quot; \u0026quot;NL\u0026quot; \u0026quot;AL\u0026quot; ... ## $ Year : int 2001 2001 2001 2001 2001 2001 2001 2001 2001 2001 ... ## $ RS : int 691 818 729 687 772 777 798 735 897 923 ... ## $ RA : int 730 677 643 829 745 701 795 850 821 906 ... ## $ W : int 75 92 88 63 82 88 83 66 91 73 ... ## $ OBP : num 0.327 0.341 0.324 0.319 0.334 0.336 0.334 0.324 0.35 0.354 ... ## $ SLG : num 0.405 0.442 0.412 0.38 0.439 0.43 0.451 0.419 0.458 0.483 ... ## $ BA : num 0.261 0.267 0.26 0.248 0.266 0.261 0.268 0.262 0.278 0.292 ... ## $ Playoffs : int 0 1 1 0 0 0 0 0 1 0 ... ## $ RankSeason : int NA 5 7 NA NA NA NA NA 6 NA ... ## $ RankPlayoffs: int NA 1 3 NA NA NA NA NA 4 NA ... ## $ G : int 162 162 162 162 161 162 162 162 162 162 ... ## $ OOBP : num 0.331 0.311 0.314 0.337 0.329 0.321 0.334 0.341 0.341 0.35 ... ## $ OSLG : num 0.412 0.404 0.384 0.439 0.393 0.398 0.427 0.455 0.417 0.48 ... ## $ RD : int -39 141 86 -142 27 76 3 -115 76 17 ... Existe uma Relação entre Run Difference e Vitórias? Só faz sentido usar uma regressão linear se é plausível a existência de uma relação linear entre as variáveis.\nplot(moneyball$RD, moneyball$W, main = \u0026quot;Vitórias vs Runs Diff.\u0026quot;, xlab = \u0026quot;Run. Diff.\u0026quot;, ylab = \u0026quot;Vitórias\u0026quot;) Realizando um teste de Correlação Existe uma correlação alta entre estas duas variáveis?\ncor(x = moneyball$RD, y = moneyball$W, method=c(\u0026quot;pearson\u0026quot;, \u0026quot;kendall\u0026quot;, \u0026quot;spearman\u0026quot;)) ## [1] 0.938515 cor.test(x = moneyball$RD, y = moneyball$W, method=c(\u0026quot;pearson\u0026quot;, \u0026quot;kendall\u0026quot;, \u0026quot;spearman\u0026quot;)) ## ## Pearson\u0026#39;s product-moment correlation ## ## data: moneyball$RD and moneyball$W ## t = 81.554, df = 900, p-value \u0026lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9302271 0.9458460 ## sample estimates: ## cor ## 0.938515 Visualizando a Correlação entre as duas variáveis Também podemos visualizar que existe uma correlação alta entre estas duas variáveis utilizando gráficos da biblioteca ggpubr.\nggpubr::ggscatter(moneyball, x = \u0026quot;RD\u0026quot;, y = \u0026quot;W\u0026quot;, add = \u0026quot;reg.line\u0026quot;, conf.int = TRUE, cor.coef = TRUE, cor.method = \u0026quot;pearson\u0026quot;, xlab = \u0026quot;Run Diff.\u0026quot;, ylab = \u0026quot;Vitórias\u0026quot;) ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39; Ok, como predizer o número de vitórias com base em Run Differences? Usando uma Regressão Linear!\n\\[ y = \\beta_0 + \\beta_1 * x + e\\]\nOu, em outras palavras…\n\\[ Vitorias = \\beta_0 + \\beta_1 * RunDiffs + e\\] Como fazer isso no R?\nmodelo_vitorias = lm(W ~ RD, data=moneyball) Analisando o Modelo para Predizer Vitórias O que isso significa: Podemos predizer o número de vitórias que um time terá a partir de um número de Home Runs. \\[ Vitorias = 80.88 + 0.1057 RunDiffs\\] summary(modelo_vitorias) ## ## Call: ## lm(formula = W ~ RD, data = moneyball) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.2662 -2.6509 0.1234 2.9364 11.6570 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 80.881375 0.131157 616.67 \u0026lt;2e-16 *** ## RD 0.105766 0.001297 81.55 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 3.939 on 900 degrees of freedom ## Multiple R-squared: 0.8808, Adjusted R-squared: 0.8807 ## F-statistic: 6651 on 1 and 900 DF, p-value: \u0026lt; 2.2e-16 Será que é são necessários 135 Runs a mais para chegar à Playoffs? A partir da regressão linear nós estimamos que $ Vitorias = 80.88 + 0.1057 RunDiffs$ e também sabemos que \\(Vitorias \u0026gt;= 95\\).\nEntão…\n\\[ 80.88 + 0.1057 RunDiffs \u0026gt;= 95\\]\nE…\n\\[RunDiffs \u0026gt;= \\frac{95 - 80.8814}{0.1058} \u0026gt;= 133,446\\]\nOu, já que estamos no R…\nRD_min = (95 - 80.8814)/0.1058 RD_min ## [1] 133.4461 OU seja, sabemos que se um time quer ir para as playoffs ele precisa fazer 133,4 Runs a mais do que seus oponentes.\nO que temos até Agora Para ir para as playoffs o time precisa de 95 vitórias ou mais. Para ter 95 vitórias, o time precisa de 133 ~ 135 Runs a mais do que os oponentes. Para isso, o time precisa: – Fazer mais Runs. – Levar menos Runs. Como Avaliar um Jogador? Percentual de Rebatidas? (Batting Average - BA) Percentual de tempo que o Jogador passa na Base? (incluindo walks) (On-Base Percentage); Slugging Percentage (SLG). O quão longe um jogador chega na sua vez de rebater; Quais destas estatísticas são mais importantes para considerar quando é necessário comprar um jogador? Recorrendo à Regressão Linear Novamente! Na nossa base de dados estas estatísticas estão indicadas nas variáveis RS (Runs Scored), On-Base Percentage (OBP), Slugging Percentage (SLG) e Batting Average (BA). str(moneyball) ## \u0026#39;data.frame\u0026#39;: 902 obs. of 16 variables: ## $ Team : chr \u0026quot;ANA\u0026quot; \u0026quot;ARI\u0026quot; \u0026quot;ATL\u0026quot; \u0026quot;BAL\u0026quot; ... ## $ League : chr \u0026quot;AL\u0026quot; \u0026quot;NL\u0026quot; \u0026quot;NL\u0026quot; \u0026quot;AL\u0026quot; ... ## $ Year : int 2001 2001 2001 2001 2001 2001 2001 2001 2001 2001 ... ## $ RS : int 691 818 729 687 772 777 798 735 897 923 ... ## $ RA : int 730 677 643 829 745 701 795 850 821 906 ... ## $ W : int 75 92 88 63 82 88 83 66 91 73 ... ## $ OBP : num 0.327 0.341 0.324 0.319 0.334 0.336 0.334 0.324 0.35 0.354 ... ## $ SLG : num 0.405 0.442 0.412 0.38 0.439 0.43 0.451 0.419 0.458 0.483 ... ## $ BA : num 0.261 0.267 0.26 0.248 0.266 0.261 0.268 0.262 0.278 0.292 ... ## $ Playoffs : int 0 1 1 0 0 0 0 0 1 0 ... ## $ RankSeason : int NA 5 7 NA NA NA NA NA 6 NA ... ## $ RankPlayoffs: int NA 1 3 NA NA NA NA NA 4 NA ... ## $ G : int 162 162 162 162 161 162 162 162 162 162 ... ## $ OOBP : num 0.331 0.311 0.314 0.337 0.329 0.321 0.334 0.341 0.341 0.35 ... ## $ OSLG : num 0.412 0.404 0.384 0.439 0.393 0.398 0.427 0.455 0.417 0.48 ... ## $ RD : int -39 141 86 -142 27 76 3 -115 76 17 ... Predizendo o Número de Runs - Modelo Completo Quanto menos Batting Average, mais Runs?! modeloruns = lm(formula = RS ~ OBP + SLG + BA, data=moneyball) summary(modeloruns) ## ## Call: ## lm(formula = RS ~ OBP + SLG + BA, data = moneyball) ## ## Residuals: ## Min 1Q Median 3Q Max ## -70.941 -17.247 -0.621 16.754 90.998 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -788.46 19.70 -40.029 \u0026lt; 2e-16 *** ## OBP 2917.42 110.47 26.410 \u0026lt; 2e-16 *** ## SLG 1637.93 45.99 35.612 \u0026lt; 2e-16 *** ## BA -368.97 130.58 -2.826 0.00482 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 24.69 on 898 degrees of freedom ## Multiple R-squared: 0.9302, Adjusted R-squared: 0.93 ## F-statistic: 3989 on 3 and 898 DF, p-value: \u0026lt; 2.2e-16 Predizendo o Número de Runs - Sem Batting Average O modelo mais simples tem menos variáveis e ainda tem um R ao quadrado alto. \\[ Runs = -804.3 + 2737.77 * OBP + 1584.91 * SLG\\]\nmodeloruns_sBA = lm(formula = RS ~ OBP + SLG, data=moneyball) summary(modeloruns_sBA) ## ## Call: ## lm(formula = RS ~ OBP + SLG, data = moneyball) ## ## Residuals: ## Min 1Q Median 3Q Max ## -70.838 -17.174 -1.108 16.770 90.036 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -804.63 18.92 -42.53 \u0026lt;2e-16 *** ## OBP 2737.77 90.68 30.19 \u0026lt;2e-16 *** ## SLG 1584.91 42.16 37.60 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 24.79 on 899 degrees of freedom ## Multiple R-squared: 0.9296, Adjusted R-squared: 0.9294 ## F-statistic: 5934 on 2 and 899 DF, p-value: \u0026lt; 2.2e-16 Predizendo o Número de Runs Allowed Com essa regressão, podemos estimar as Runs Permitidas com a equação: \\[ Runs Permitidas = -837 + 2913.6 * OOBP + 1514.29 * OSLG\\]\nmodelorunsallowed = lm(formula = RA ~ OOBP + OSLG, data=moneyball) summary(modelorunsallowed) ## ## Call: ## lm(formula = RA ~ OOBP + OSLG, data = moneyball) ## ## Residuals: ## Min 1Q Median 3Q Max ## -82.397 -15.178 -0.129 17.679 60.955 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -837.38 60.26 -13.897 \u0026lt; 2e-16 *** ## OOBP 2913.60 291.97 9.979 4.46e-16 *** ## OSLG 1514.29 175.43 8.632 2.55e-13 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 25.67 on 87 degrees of freedom ## (812 observations deleted due to missingness) ## Multiple R-squared: 0.9073, Adjusted R-squared: 0.9052 ## F-statistic: 425.8 on 2 and 87 DF, p-value: \u0026lt; 2.2e-16 Agora vem a parte legal Com os nossos modelos, agora é possível tentar predizer quantos jogos o Oakland A’s vai ganhar em um determinado ano. Estamos tentando predizer quantos jogos o time vai ganhar antes da temporada começar, com o objetivo de suportar a decisão sobre quais jogadores queremos comprar. Pressuposto # 1: Performance passada dos jogadores do time que estamos montando tem correlação com a performance futura. Pressuposto # 2: A análise assume que haverão poucas lesões. Pressuposto # 3: Podemos estimar estatísticas para 2002 usando estatísticas dos jogadores coletadas em 2001. Estimando Runs para 2002 Com base na temporada de 2001, com o grupo que tivemos sabemos que a média do OBP é 0.339, e do SLG é 0.430. Nossa Regressão para Runs foi $ Runs = -804.3 + 2737.77 * OBP + 1584.91 * SLG$. Então a Estimativa de Runs é.. Runs = -804.3 + 2737.77 * 0.339 + 1584.91 * 0.430 Runs ## [1] 805.3153 Estimando Runs Allowed para 2002 Com base na temporada de 2001, com o grupo que tivemos sabemos que a média do OOBP é 0.307, e do OSLG é 0.373. Nossa Regressão para Runs foi $ Runs Permitidas = -837 + 2913.6 * OOBP + 1514.29 * OSLG$. Podemos fazer o mesmo para Runs Allowed RunsAllowed = -837 + 2913.6 * 0.307 + 1514.29 * 0.373 RunsAllowed ## [1] 622.3054 Quantos Jogos Esperamos Ganhar com esse Time? Nosso modelo de vitórias diz que $ Vitorias = 80.88 + 0.1057 RunDiffs$. Então.. Vitorias = 80.88 + 0.1057 * (Runs - RunsAllowed) if (Vitorias \u0026gt;= 95) { paste(\u0026quot;Esse time deve chegar nas Playoffs com \u0026quot;, Vitorias, \u0026quot; vitórias.\u0026quot;) } else { paste(\u0026quot;Compre outros Jogadores, este time não chega nas playoffs com apenas \u0026quot;, Vitorias, \u0026quot; vitórias!\u0026quot;) } ## [1] \u0026quot;Esse time deve chegar nas Playoffs com 100.224152772 vitórias.\u0026quot; A abordagem de Paul foi parecida com essa. A hora da verdade Nosso modelo serve para alguma coisa? Variável Nosso Modelo Modelo do Paul Realizado Runs 805 800 - 820 800 Runs Allowed 622 650 - 670 653 Vitórias 100 93 - 97 103 O Oakland A’s ganhou 20 jogos em sequência nesse ano, mas não ganhou o campeonato; O Oakland A’s conseguiu ir para as Playoffs mais uma vez! Como Aprender Mais? github.com; datacamp.com; stackoverflow.com; udacity.com; edx.org; coursera.org; ","date":1507593600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507593600,"objectID":"d64af70ef4b318c6e78a632b4d965907","permalink":"https://www.pedrodelima.com/post/introducao-ao-r-aprendendo-com-o-moneyball/","publishdate":"2017-10-10T00:00:00Z","relpermalink":"/post/introducao-ao-r-aprendendo-com-o-moneyball/","section":"post","summary":"Este post corresponde aos slides do Mini-Curso “Introdução ao R - Aprendendo com o Moneyball”, realizado em 2017 no SIGEPRO. Cada um dos títulos neste post correspondia a um slide.","tags":[],"title":"Introdução ao R - Aprendendo com o Moneyball","type":"post"},{"authors":null,"categories":null,"content":"This online tool shows the effect that improving diet quality in the United States could have on health and economic outcomes, such as the prevalence of diet-related illness, health care spending, and labor force participation, over a 30-year period. The user can explore the effects of a dietary improvement on several outcomes, across different metrics and population sub-groups.\n","date":1459036800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459036800,"objectID":"81a014590d9aaceeb46932d1c040f0ef","permalink":"https://www.pedrodelima.com/project/diet-scenarios/","publishdate":"2016-03-27T00:00:00Z","relpermalink":"/project/diet-scenarios/","section":"project","summary":"Helps people understand effects of diet improvement on long-term health and economic outcomes.","tags":["Tool"],"title":"The \"Diet Tool\"","type":"project"},{"authors":null,"categories":null,"content":"","date":1453852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1453852800,"objectID":"d983d10d2205064930a0462d719a5a70","permalink":"https://www.pedrodelima.com/project/arena2r/","publishdate":"2016-01-27T00:00:00Z","relpermalink":"/project/arena2r/","section":"project","summary":"An R package for Arena Users. Creates plots and summary statistics from Arena output files.","tags":["R Package"],"title":"Arena2R","type":"project"}]